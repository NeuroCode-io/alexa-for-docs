{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.1 64-bit ('.venv')",
   "metadata": {
    "interpreter": {
     "hash": "5bdca235408bc63495fd8719d073449338626e85bb6ce08bacdb45b63101d775"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.file_converter.pdf import PDFToTextConverter\n",
    "converter = PDFToTextConverter(remove_numeric_tables=True, valid_languages=[\"en\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "book = converter.convert(file_path=\"/home/elena/Downloads/9781839217579-THE_DEEP_LEARNING_WITH_KERAS_WORKSHOP_SECOND_EDITION.pdf\", meta=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(dict, 2, dict_keys(['text', 'meta']), 579763, NoneType)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "type(book), len(book), book.keys(), len(book[\"text\"]), type(book[\"meta\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Buckingham,\\nMegan Carlisle, Mahesh Dhyani, Manasa Kumar, Alex Mazonowicz, Bridget Neale,\\nDominic Pereira, Shiny Poojary, Abhishek Rane, Brendan Rodrigues, Mugdha Sawarkar,\\nErol Staveley, Ankita Thakur, Nitesh Thakur, and Jonathan Wray\\nFirst published: April 2019\\nSecond edition: February 2020\\nProduction reference: 1270220\\nPublished by Packt Publishing Ltd.\\nLivery Place, 35 Livery Street\\n\\x0cTable of Contents\\nPreface\\u202f\\t\\u202fi\\nChapter 1: Introduction to Machine Learning with Keras\\u202f\\t\\u202f1\\nIntroduction ..................................................................................................... 2\\nData Representation ...................................................................................... 4\\nTables of Data ........................................................................................................ 4\\nLoading Data ......................................................................................................... 5\\nExercise 1.01: Loading a Dataset from\\nthe UCI Machine Learning Repository ................................................................ 7\\n\\nData Preprocessing ....................................................................................... 10\\nExercise 1.02: Cleaning the Data ....................................................................... 12\\nAppropriate Representation of the Data ......................................................... 22\\nExercise 1.03: Appropriate Representation of the Data ................................ 23\\n\\nLife Cycle of Model Creation ........................................................................ 26\\nMachine Learning Libraries ............................................................................... 26\\n\\nscikit-learn ..................................................................................................... 27\\nKeras ............................................................................................................... 29\\nAdvantages of Keras...................................................................................... 30\\nDisadvantages of Keras................................................................................. 30\\nMore than Building Models ............................................................................... 31\\n\\nModel Training .............................................................................................. 32\\nClassifiers and Regression Models ................................................................... 32\\nClassification Tasks ............................................................................................. 34\\nRegression Tasks ................................................................................................. 35\\nTraining Datasets and Test Datasets ................................................................ 35\\nModel Evaluation Metrics .................................................................................. 36\\n\\x0cExercise 1.04: Creating a Simple Model ........................................................... 38\\n\\nModel Tuning ................................................................................................. 42\\nBaseline Models .................................................................................................. 42\\nExercise 1.05: Determining a Baseline Model ................................................. 42\\nRegularization ..................................................................................................... 4'"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "book[\"text\"][1500:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.preprocessor.preprocessor import PreProcessor\n",
    "\n",
    "# we can use PreProcessor to split by passage, sentence and word\n",
    "# passage wil split data, i.e. book in to dicts; one with meta _split_id 0 and the other _split_id 1 \n",
    "# sentence with split length 260 will split book in 14 dicts; key meta numerating those dicts (from 0 to 13)\n",
    "# word with split length 260 will split book into 329 dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /home/elena/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from haystack.preprocessor.preprocessor import PreProcessor\n",
    "\n",
    "processor_passage = PreProcessor(clean_empty_lines=True,\n",
    "                         clean_whitespace=True,\n",
    "                         clean_header_footer=True,\n",
    "                         split_by=\"passage\",\n",
    "                         split_respect_sentence_boundary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "passage = processor_passage.process(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(list, 2, 2, 2, dict, dict_keys(['text', 'meta']), dict_keys(['text', 'meta']))"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "type(passage), len(passage), len(passage[0]), len(passage[1]), type(passage[0]), passage[0].keys(), passage[1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(369553, 206463, 576016, {'_split_id': 0}, {'_split_id': 1})"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "len(passage[0][\"text\"]), len(passage[1][\"text\"]), len(passage[0][\"text\"])+len(passage[1][\"text\"]), passage[0][\"meta\"], passage[1][\"meta\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /home/elena/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "processor_sentence = PreProcessor(clean_empty_lines=True,\n",
    "                         clean_whitespace=True,\n",
    "                         clean_header_footer=True,\n",
    "                         split_by=\"sentence\",\n",
    "                         split_length = 260,\n",
    "                         split_respect_sentence_boundary=True)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = processor_sentence.process(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(list, 14, dict, dict_keys(['text', 'meta']), 53981, 30844)"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "type(sent), len(sent), type(sent[0]), sent[0].keys(), len(sent[0][\"text\"]), len(sent[9][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /home/elena/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "processor_word = PreProcessor(clean_empty_lines=True,\n",
    "                         clean_whitespace=True,\n",
    "                         clean_header_footer=True,\n",
    "                         split_by=\"word\",\n",
    "                         split_length = 260,\n",
    "                         split_respect_sentence_boundary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A sentence found with word count higher than the split length.\n"
     ]
    }
   ],
   "source": [
    "word=processor_word.process(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(list, 329, dict_keys(['text', 'meta']), {'_split_id': 0})"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "type(word), len(word), word[0].keys(), word[0][\"meta\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.document_store.faiss import FAISSDocumentStore\n",
    "\n",
    "document_store = FAISSDocumentStore()\n",
    "document_store.delete_all_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store.write_documents(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "12/02/2020 09:27:43 - WARNING - farm.modeling.language_model -   Could not automatically detect from language model name what language it is. \n",
      "\t We guess it's an *ENGLISH* model ... \n",
      "\t If not: Init the language model by supplying the 'language' param.\n",
      "12/02/2020 09:27:51 - WARNING - farm.modeling.language_model -   Could not automatically detect from language model name what language it is. \n",
      "\t We guess it's an *ENGLISH* model ... \n",
      "\t If not: Init the language model by supplying the 'language' param.\n"
     ]
    }
   ],
   "source": [
    "from haystack.retriever.dense import DensePassageRetriever\n",
    "retriever = DensePassageRetriever(document_store=document_store,\n",
    "                                  query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n",
    "                                  passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\n",
    "                                  max_seq_len_query=64,\n",
    "                                  max_seq_len_passage=256,\n",
    "                                  batch_size=16,\n",
    "                                  use_gpu=False,\n",
    "                                  embed_title=True,\n",
    "                                  use_fast_tokenizers=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ls | 403\\n8. Check the shape of the image. It should be (1,1000):\\ny_pred.shape\\n\\nThe preceding code provides the following output:\\n\\n9. Select the top five probabilities of what our image is using the decode_\\npredictions function and by passing the predictor variable, y_pred, as the\\nargument and the top number of predictions and corresponding labels:\\nfrom keras.applications.resnet50 import decode_predictions\\n\\nThe preceding code provides the following output:\\n\\nThe first column of the array is an internal code number. The second is the label,\\nwhile the third is the probability of the image matching the label. 10. Put the predictions in a human-readable format. Print the most probable label\\nfrom the output from the result of the decode_predictions function:\\nlabel = decode_predictions(y_pred)\\n# Most likely result is retrieved, for example, the highest\\nprobability\\ndecoded_label = label[0][0]\\n# The classification is printed\\n\\nThe preceding code produces the following output:\\n\\n404 | Appendix\\n\\nChapter 9: Sequential Modeling with Recurrent Neural Networks\\nActivity 9.01: Predicting the Trend of Amazon\\'s Stock Price Using an LSTM\\nwith 50 Units (Neurons)\\nIn this activity, we will examine the stock price of Amazon for the last 5 years—from\\nJanuary 1, 2014, to December 31, 2018. In doing so, we will try to predict and forecast\\nthe company\\'s future trend for January 2019 using an RNN and LSTM. We have the actual\\nvalues for January 2019, so we can compare our predictions to the actual values later. Follow these steps to complete this activity:\\n1. Import the required libraries:\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nfrom tensorflow import random\\n\\n2. Import the dataset using the pandas read_csv function and look at the first five\\nrows of the dataset using the head method:\\ndataset_training = pd.read_csv(\\'../AMZN_train.csv\\')\\ndataset_training.head()\\n\\nThe following figure shows the output of the preceding code:\\n\\nFigure 9.24: The first five rows of the dataset\\n\\n3. We are going to make our prediction using the Open stock price; therefore, select\\nthe Open stock price column from the dataset and print the values:\\ntraining_data = dataset_training[[\\'Open\\']].values\\ntraining_data\\n\\nChapter 9: Sequential Modeling with Recurrent Neural Networks | 405\\nThe preceding code produces the following output:\\n...,\\n\\n4. Then, perform feature scaling by normalizing the data using MinMaxScaler and\\nsetting the range of the features so that they have a minimum value of zero and\\na maximum value of one. Use the fit_transform method of the scaler on the\\ntraining data:\\nfrom sklearn.preprocessing import MinMaxScaler\\nsc = MinMaxScaler(feature_range = (0, 1))\\ntraining_data_scaled = sc.fit_transform(training_data)\\ntraining_data_scaled\\n\\nThe preceding code produces the following output:\\n...,\\n\\n5. Create the data to get 60 timestamps from the current instance. We chose 60 here\\nas it will give us a sufficient number of previous instances in order to understand\\nthe trend; technically, this can be any number, but 60 is the optimal value. Additionally, the upper bound value here is 1258, which is the index or count of\\nrows (or records) in the training set:\\nX_train = []\\ny_train = []\\nfor i in range(60, 1258):\\nX_train, y_train = np.array(X_train), np.array(y_train)\\n\\n406 | Appendix\\n6. Reshape the data to add an extra dimension to the end of X_train using NumPy\\'s\\nreshape function:\\n\\n7. Import the following libraries to build the RNN:\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense, LSTM, Dropout\\n\\n8. Set the seed and initiate the sequential model, as follows:\\nseed = 1\\nnp.random.seed(seed)\\nrandom.set_seed(seed)\\nmodel = Sequential()\\n\\n9. Add an LSTM layer to the network with 50 units, set the return_sequences\\nargument to True, and set the input_shape argument to (X_train.shape[1],\\n1). Add three additional LSTM layers, each with 50 units, and set the return_\\nsequences argument to True for the first two. Add a final output layer of size 1:\\nmodel.add(LSTM(units = 50, return_sequences = True, input_shape = (X_\\n# Adding a second LSTM layer\\nmodel.add(LSTM(units = 50, return_sequences = True))\\n# Adding a third LSTM layer\\nmodel.add(LSTM(units = 50, return_sequences = True))\\n# Adding a fourth LSTM layer\\nmodel.add(LSTM(units = 50))\\n# Adding the output layer\\nmodel.add(Dense(units = 1))\\n\\n10. Compile the network with an adam optimizer and use Mean Squared Error for\\nthe loss. Fit the model to the training data for 100 epochs with a batch size of 32:\\n# Compiling the RNN\\nmodel.compile(optimizer = \\'adam\\', loss = \\'mean_squared_error\\')\\n# Fitting the RNN to the Training set\\nmodel.fit(X_train, y_train, epochs = 100, batch_size = 32)\\n\\nChapter 9: Sequential Modeling with Recurrent Neural Networks | 407\\n11. Load and process the test data (which is treated as actual data here) and select the\\ncolumn representing the value of Open stock data:\\ndataset_testing = pd.read_csv(\\'../AMZN_test.csv\\')\\nactual_stock_price = dataset_testing[[\\'Open\\']].values\\nactual_stock_price\\n\\n12. Concatenate the data since we will need 60 previous instances to get the stock\\nprice for each day. Therefore, we will need both the training and test data:\\ntotal_data = pd.concat((dataset_training[\\'Open\\'], dataset_\\ntesting[\\'Open\\']), axis = 0)\\n\\n13. Reshape and scale the input to prepare the test data. Note that we are predicting\\nthe January monthly trend, which has 21 financial days, so in order to prepare the\\ntest set, we take the lower bound value as 60 and the upper bound value as 81. This\\nensures that the difference of 21 is maintained:\\ninputs = total_data[len(total_data) - len(dataset_testing) - 60:]. values\\ninputs = inputs.reshape(-1,1)\\ninputs = sc.transform(inputs)\\nX_test = []\\nfor i in range(60, 81):\\nX_test = np.array(X_test)\\npredicted_stock_price = model.predict(X_test)\\npredicted_stock_price = sc.inverse_transform(predicted_stock_price)\\n\\n14. Visualize the results by plotting the actual stock price and plotting the predicted\\nstock price:\\n# Visualizing the results\\nplt.plot(actual_stock_price, color = \\'green\\', label = \\'Real Amazon\\nStock Price\\',ls=\\'--\\')\\nplt.plot(predicted_stock_price, color = \\'red\\', label = \\'Predicted\\nAmazon Stock Price\\',ls=\\'-\\')\\nplt.title(\\'Predicted Stock Price\\')\\nplt.xlabel(\\'Time in days\\')\\nplt.ylabel(\\'Real Stock Price\\')\\nplt.legend()\\nplt.show()\\n\\nPlease note that your results may differ slightly from the actual stock price\\nof Amazon. 408 | Appendix\\nExpected output:\\n\\nFigure 9.25: Real versus predicted stock prices\\n\\nAs shown in the preceding plot, the trends of the predicted and real prices are\\npretty much the same; the line has the same peaks and troughs. This is possible\\nbecause of LSTM\\'s ability to remember sequenced data. A traditional feedforward\\nneural network would not have been able to forecast this result. This is the true\\npower of LSTM and RNNs. Activity 9.02: Predicting Amazon\\'s Stock Price with Added Regularization\\nIn this activity, we will examine the stock price of Amazon over the last 5 years, from\\nJanuary 1, 2014, to December 31, 2018. In doing so, we will try to predict and forecast\\nthe company\\'s future trend for January 2019 using RNNs and an LSTM. We have the\\nactual values for January 2019, so we will be able to compare our predictions with the\\nactual values later. Initially, we predicted the trend of Amazon\\'s stock price using an\\nLSTM with 50 units (or neurons). In this activity, we will also add dropout regularization\\nand compare the results with Activity 9.01, Predicting the Trend of Amazon\\'s Stock Price\\nUsing an LSTM with 50 Units (Neurons). Follow these steps to complete this activity:\\n1. Import the required libraries:\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nfrom tensorflow import random\\n\\nChapter 9: Sequential Modeling with Recurrent Neural Networks | 409\\n2. Import the dataset using the pandas read_csv function and look at the first five\\nrows of the dataset using the head method:\\ndataset_training = pd.read_csv(\\'../AMZN_train.csv\\')\\ndataset_training.head()\\n\\n3. We are going to make our prediction using the Open stock price; therefore, select\\nthe Open stock price column from the dataset and print the values:\\ntraining_data = dataset_training[[\\'Open\\']].values\\ntraining_data\\n\\nThe preceding code produces the following output:\\n...,\\n\\n4. Then, perform feature scaling by normalizing the data using MinMaxScaler\\nand setting the range of the features so that they have a minimum value of 0 and\\na maximum value of one. Use the fit_transform method of the scaler on the\\ntraining data:\\nfrom sklearn.preprocessing import MinMaxScaler\\nsc = MinMaxScaler(feature_range = (0, 1))\\ntraining_data_scaled = sc.fit_transform(training_data)\\ntraining_data_scaled\\n\\nThe preceding code produces the following output:\\n...,\\n\\n410 | Appendix\\n5. Create the data to get 60 timestamps from the current instance. We chose 60 here\\nas it will give us a sufficient number of previous instances in order to understand\\nthe trend; technically, this can be any number, but 60 is the optimal value. Additionally, the upper bound value here is 1258, which is the index or count of\\nrows (or records) in the training set:\\nX_train = []\\ny_train = []\\nfor i in range(60, 1258):\\nX_train, y_train = np.array(X_train), np.array(y_train)\\n\\n6. Reshape the data to add an extra dimension to the end of X_train using NumPy\\'s\\nreshape function:\\n\\n7. Import the following Keras libraries to build the RNN:\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense, LSTM, Dropout\\n\\n8. Set the seed and initiate the sequential model, as follows:\\nseed = 1\\nnp.random.seed(seed)\\nrandom.set_seed(seed)\\nmodel = Sequential()\\n\\n9. Add an LSTM layer to the network with 50 units, set the return_sequences\\nargument to True, and set the input_shape argument to (X_train.shape[1],\\n1). Add dropout to the model with rate=0.2. Add three additional LSTM layers,\\neach with 50 units, and set the return_sequences argument to True for the first\\ntwo. After each LSTM layer, add a dropout with rate=0.2. Add a final output layer\\nof size 1:\\nmodel.add(LSTM(units = 50, return_sequences = True, input_shape = (X_\\n# Adding a second LSTM layer and some Dropout regularization\\nmodel.add(LSTM(units = 50, return_sequences = True))\\n\\nChapter 9: Sequential Modeling with Recurrent Neural Networks | 411\\n# Adding a third LSTM layer and some Dropout regularization\\nmodel.add(LSTM(units = 50, return_sequences = True))\\n# Adding a fourth LSTM layer and some Dropout regularization\\nmodel.add(LSTM(units = 50))\\n# Adding the output layer\\nmodel.add(Dense(units = 1))\\n\\n10. Compile the network with an adam optimizer and use Mean Squared Error for\\nthe loss. Fit the model to the training data for 100 epochs with a batch size of 32:\\n# Compiling the RNN\\nmodel.compile(optimizer = \\'adam\\', loss = \\'mean_squared_error\\')\\n# Fitting the RNN to the Training set\\nmodel.fit(X_train, y_train, epochs = 100, batch_size = 32)\\n\\n11. Load and process the test data (which is treated as actual data here) and select the\\ncolumn representing the value of Open stock data:\\ndataset_testing = pd.read_csv(\\'../AMZN_test.csv\\')\\nactual_stock_price = dataset_testing[[\\'Open\\']].values\\nactual_stock_price\\n\\n12. Concatenate the data since we will need 60 previous instances to get the stock\\nprice for each day. Therefore, we will need both the training and test data:\\ntotal_data = pd.concat((dataset_training[\\'Open\\'], dataset_\\ntesting[\\'Open\\']), axis = 0)\\n\\n13. Reshape and scale the input to prepare the test data. Note that we are predicting\\nthe January monthly trend, which has 21 financial days, so in order to prepare the\\ntest set, we take the lower bound value as 60 and the upper bound value as 81. This\\nensures that the difference of 21 is maintained:\\ninputs\\nvalues\\ninputs\\ninputs\\nX_test\\n\\n= total_data[len(total_data) - len(dataset_testing) - 60:]. = sc.transform(inputs)\\n= []\\n\\n412 | Appendix\\nfor i in range(60, 81):\\nX_test = np.array(X_test)\\npredicted_stock_price = model.predict(X_test)\\npredicted_stock_price = sc.inverse_transform(predicted_stock_price)\\n\\n14. Visualize the results by plotting the actual stock price and plotting the predicted\\nstock price:\\n# Visualizing the results\\nplt.plot(actual_stock_price, color = \\'green\\', label = \\'Real Amazon\\nStock Price\\',ls=\\'--\\')\\nplt.plot(predicted_stock_price, color = \\'red\\', label = \\'Predicted\\nAmazon Stock Price\\',ls=\\'-\\')\\nplt.title(\\'Predicted Stock Price\\')\\nplt.xlabel(\\'Time in days\\')\\nplt.ylabel(\\'Real Stock Price\\')\\nplt.legend()\\nplt.show()\\n\\nPlease note that your results may differ slightly to the actual stock price. Expected output:\\n\\nFigure 9.26: Real versus predicted stock prices\\n\\nChapter 9: Sequential Modeling with Recurrent Neural Networks | 413\\nIn the following figure, the first plot displays the predicted output of the model with\\nregularization from Activity 9.02, and the second displays the predicted output without\\nregularization from Activity 9.01. As you can see, adding dropout regularization does not\\nfit the data as accurately. So, in this case, it is better not to use regularization, or to use\\ndropout regularization with a lower dropout rate:\\n\\nFigure 9.27: Comparing the results of Activity 9.01 and Activity 9.02\\n\\nActivity 9.03: Predicting the Trend of Amazon\\'s Stock Price Using an LSTM\\nwith an Increasing Number of LSTM Neurons (100 Units)\\nIn this activity, we will examine the stock price of Amazon over the last 5 years, from\\nJanuary 1, 2014, to December 31, 2018. We will try to predict and forecast the company\\'s\\nfuture trend for January 2019 using RNNs with four LSTM layers, each with 100 units. We\\nhave the actual values for January 2019, so we will be able to compare our predictions\\nwith the actual values later. You can also compare the output difference with Activity\\n9.01, Predicting the Trend of Amazon\\'s Stock Price Using an LSTM with 50 Units\\n(Neurons). Follow these steps to complete this activity:\\n1. Import the required libraries:\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nfrom tensorflow import random\\n\\n2. Import the dataset using the pandas read_csv function and look at the first five\\nrows of the dataset using the head method:\\ndataset_training = pd.read_csv(\\'../AMZN_train.csv\\')\\ndataset_training.head()\\n\\n414 | Appendix\\n3. We are going to make our prediction using the Open stock price; therefore, select\\nthe Open stock price column from the dataset and print the values:\\ntraining_data = dataset_training[[\\'Open\\']].values\\ntraining_data\\n\\n4. Then, perform feature scaling by normalizing the data using MinMaxScaler and\\nsetting the range of the features so that they have a minimum value of zero and\\na maximum value of one. Use the fit_transform method of the scaler on the\\ntraining data:\\nfrom sklearn.preprocessing import MinMaxScaler\\nsc = MinMaxScaler(feature_range = (0, 1))\\ntraining_data_scaled = sc.fit_transform(training_data)\\ntraining_data_scaled\\n\\n5. Create the data to get 60 timestamps from the current instance. We chose 60 here\\nas it will give us a sufficient number of previous instances in order to understand\\nthe trend; technically, this can be any number, but 60 is the optimal value. Additionally, the upper bound value here is 1258, which is the index or count of\\nrows (or records) in the training set:\\nX_train = []\\ny_train = []\\nfor i in range(60, 1258):\\nX_train, y_train = np.array(X_train), np.array(y_train)\\n\\n6. Reshape the data to add an extra dimension to the end of X_train using NumPy\\'s\\nreshape function:\\n\\n7. Import the following Keras libraries to build the RNN:\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense, LSTM, Dropout\\n\\n8. Set the seed and initiate the sequential model:\\nseed = 1\\nnp.random.seed(seed)\\nrandom.set_seed(seed)\\nmodel = Sequential()\\n\\nChapter 9: Sequential Modeling with Recurrent Neural Networks | 415\\n9. Add an LSTM layer to the network with 100 units, set the return_sequences\\nargument to True, and set the input_shape argument to (X_train.shape[1],\\n1). Add three additional LSTM layers, each with 100 units, and set the return_\\nsequences argument to True for the first two. Add a final output layer of size 1:\\nmodel.add(LSTM(units = 100, return_sequences = True, input_shape = (X_\\n# Adding a second LSTM layer\\nmodel.add(LSTM(units = 100, return_sequences = True))\\n# Adding a third LSTM layer\\nmodel.add(LSTM(units = 100, return_sequences = True))\\n# Adding a fourth LSTM layer\\nmodel.add(LSTM(units = 100))\\n# Adding the output layer\\nmodel.add(Dense(units = 1))\\n\\n10. Compile the network with an adam optimizer and use Mean Squared Error for\\nthe loss. Fit the model to the training data for 100 epochs with a batch size of 32:\\n# Compiling the RNN\\nmodel.compile(optimizer = \\'adam\\', loss = \\'mean_squared_error\\')\\n# Fitting the RNN to the Training set\\nmodel.fit(X_train, y_train, epochs = 100, batch_size = 32)\\n\\n11. Load and process the test data (which is treated as actual data here) and select the\\ncolumn representing the value of open stock data:\\ndataset_testing = pd.read_csv(\\'../AMZN_test.csv\\')\\nactual_stock_price = dataset_testing[[\\'Open\\']].values\\nactual_stock_price\\n\\n12. Concatenate the data since we will need 60 previous instances to get the stock\\nprice for each day. Therefore, we will need both the training and test data:\\ntotal_data = pd.concat((dataset_training[\\'Open\\'], dataset_\\ntesting[\\'Open\\']), axis = 0)\\n\\n416 | Appendix\\n13. Reshape and scale the input to prepare the test data. Note that we are predicting\\nthe January monthly trend, which has 21 financial days, so in order to prepare the\\ntest set, we take the lower bound value as 60 and the upper bound value as 81. This\\nensures that the difference of 21 is maintained:\\ninputs = total_data[len(total_data) - len(dataset_testing) - 60:]. values\\ninputs = inputs.reshape(-1,1)\\ninputs = sc.transform(inputs)\\nX_test = []\\nfor i in range(60, 81):\\nX_test = np.array(X_test)\\npredicted_stock_price = model.predict(X_test)\\npredicted_stock_price = sc.inverse_transform(predicted_stock_price)\\n\\n14. Visualize the results by plotting the actual stock price and plotting the predicted\\nstock price:\\nplt.plot(actual_stock_price, color = \\'green\\', label = \\'Actual Amazon\\nStock Price\\',ls=\\'--\\')\\nplt.plot(predicted_stock_price, color = \\'red\\', label = \\'Predicted\\nAmazon Stock Price\\',ls=\\'-\\')\\nplt.title(\\'Predicted Stock Price\\')\\nplt.xlabel(\\'Time in days\\')\\nplt.ylabel(\\'Real Stock Price\\')\\nplt.legend()\\nplt.show()\\n\\nPlease note that your results may differ slightly from the actual stock price. Chapter 9: Sequential Modeling with Recurrent Neural Networks | 417\\nExpected output:\\n\\nFigure 9.28: Real versus predicted stock prices\\n\\nSo, if we compare the results of the LSTM with 50 units (from Activity 9.01, Predicting\\nthe Trend of Amazon\\'s Stock Price Using an LSTM with 50 Units (Neurons)) and the LSTM\\nwith 100 units in this activity, we get trends with 100 units. Also, note that when we run\\nthe LSTM with 100 units, it takes more computational time than the LSTM with 50 units. A trade-off needs to be considered in such cases:\\n\\nFigure 9.29: Comparing the real versus predicted stock price with 50 and 100 units\\n\\n>\\nIndex\\n\\nAbout\\nAll major keywords used in this book are captured alphabetically in this section. Each one is\\naccompanied by the page number of where they appear. A\\n\\nB\\n\\nC\\n\\nD\\n\\nE\\n\\nF\\n\\nG\\n\\nI\\n\\nL\\n\\nM\\n\\nO\\n\\nP\\n\\nR\\n\\nS\\n\\nT\\n\\nU\\n\\nV\\n\\nW\\n\\nX\\n\\nY\\n\\nZ', 'label': 'positive', 'external_id': '56d25733-8be7-4381-9896-2360d1f1643b'}]}\n",
      "12/02/2020 09:27:51 - ERROR - farm.data_handler.processor -   Error message: Truncation error: Specified max length is too low to respect the various constraints\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-a042bbe71db2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdocument_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretriever\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/neurocode/alexa-for-docs/.venv/lib/python3.8/site-packages/haystack/document_store/faiss.py\u001b[0m in \u001b[0;36mupdate_embeddings\u001b[0;34m(self, retriever, index)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Updating embeddings for {len(documents)} docs...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretriever\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_passages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/neurocode/alexa-for-docs/.venv/lib/python3.8/site-packages/haystack/retriever/dense.py\u001b[0m in \u001b[0;36membed_passages\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    200\u001b[0m             \"external_id\": d.id}]\n\u001b[1;32m    201\u001b[0m         } for d in docs]\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpassages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpassage_tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"passages\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/neurocode/alexa-for-docs/.venv/lib/python3.8/site-packages/haystack/retriever/dense.py\u001b[0m in \u001b[0;36m_get_predictions\u001b[0;34m(self, dicts, tokenizer)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mof\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m\"passages\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"query\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \"\"\"\n\u001b[0;32m--> 153\u001b[0;31m         dataset, tensor_names, baskets = self.processor.dataset_from_dicts(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0mdicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_baskets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         )\n",
      "\u001b[0;32m~/Projects/neurocode/alexa-for-docs/.venv/lib/python3.8/site-packages/farm/data_handler/processor.py\u001b[0m in \u001b[0;36mdataset_from_dicts\u001b[0;34m(self, dicts, indices, return_baskets)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_samples_in_baskets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_featurize_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/neurocode/alexa-for-docs/.venv/lib/python3.8/site-packages/farm/data_handler/processor.py\u001b[0m in \u001b[0;36m_featurize_samples\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_featurize_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbasket\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbaskets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbasket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                     \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sample_to_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "document_store.update_embeddings(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "12/02/2020 09:28:01 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n",
      "12/02/2020 09:28:01 - INFO - farm.infer -   Could not find `deepset/roberta-base-squad2` locally. Try to download from model hub ...\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at deepset/roberta-base-squad2 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "12/02/2020 09:28:04 - WARNING - farm.modeling.language_model -   Could not automatically detect from language model name what language it is. \n",
      "\t We guess it's an *ENGLISH* model ... \n",
      "\t If not: Init the language model by supplying the 'language' param.\n",
      "12/02/2020 09:28:13 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n",
      "12/02/2020 09:28:13 - INFO - farm.infer -   Got ya 3 parallel workers to do inference ...\n",
      "12/02/2020 09:28:13 - INFO - farm.infer -    0    0    0 \n",
      "12/02/2020 09:28:13 - INFO - farm.infer -   /w\\  /w\\  /w\\\n",
      "12/02/2020 09:28:13 - INFO - farm.infer -   /'\\  / \\  /'\\\n",
      "12/02/2020 09:28:13 - INFO - farm.infer -       \n"
     ]
    }
   ],
   "source": [
    "from haystack.reader.farm import FARMReader\n",
    "farm_reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder = Finder(farm_reader, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.86 Batches/s]\n",
      "12/02/2020 09:28:20 - INFO - haystack.finder -   Got 0 candidates from retriever\n",
      "12/02/2020 09:28:20 - INFO - haystack.finder -   Retriever did not return any documents. Skipping reader ...\n"
     ]
    }
   ],
   "source": [
    "prediction = finder.get_answers(question=\"layer\", top_k_retriever=1, top_k_reader=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.utils import print_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print_answers(prediction, details=\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "12/02/2020 09:28:50 - INFO - elasticsearch -   HEAD http://localhost:9200/document [status:200 request:0.042s]\n",
      "12/02/2020 09:28:50 - INFO - elasticsearch -   GET http://localhost:9200/document [status:200 request:0.003s]\n",
      "12/02/2020 09:28:50 - INFO - elasticsearch -   PUT http://localhost:9200/document/_mapping [status:200 request:0.030s]\n",
      "12/02/2020 09:28:50 - INFO - elasticsearch -   HEAD http://localhost:9200/label [status:200 request:0.004s]\n"
     ]
    }
   ],
   "source": [
    "from haystack.document_store.elasticsearch import ElasticsearchDocumentStore\n",
    "document_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\", index=\"document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'document'"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "document_store.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "12/02/2020 09:33:11 - INFO - elasticsearch -   POST http://localhost:9200/docuement/_delete_by_query [status:404 request:0.036s]\n"
     ]
    }
   ],
   "source": [
    "document_store.delete_all_documents(index=\"docuement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "12/02/2020 09:33:14 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:0.595s]\n"
     ]
    }
   ],
   "source": [
    "document_store.write_documents(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.retriever.sparse import ElasticsearchRetriever\n",
    "retriever = ElasticsearchRetriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder = Finder(farm_reader, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "12/02/2020 09:33:20 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.035s]\n",
      "12/02/2020 09:33:20 - INFO - haystack.finder -   Got 10 candidates from retriever\n",
      "12/02/2020 09:33:20 - INFO - haystack.finder -   Reader is looking for detailed answer in 143935 chars ...\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.88s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.20 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.25 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 2/2 [00:26<00:00, 13.15s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 2/2 [00:25<00:00, 12.88s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.08 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 2/2 [00:27<00:00, 13.96s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 2/2 [00:33<00:00, 16.53s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.15s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:02<00:00,  2.73s/ Batches]\n"
     ]
    }
   ],
   "source": [
    "prediction = finder.get_answers(question=\"What is RNN?\", top_k_retriever=10, top_k_reader=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[   {   'answer': 'Recurrent Neural Networks',\n        'context': ' chapter, you will\\n'\n                   'be able to build sequential models, explain Recurrent '\n                   'Neural Networks (RNNs),\\n'\n                   'describe the vanishing gradient problem, and implemen',\n        'score': 16.782752990722656},\n    {   'answer': 'Recurrent Neural Networks',\n        'context': ' chapter, you will\\n'\n                   'be able to build sequential models, explain Recurrent '\n                   'Neural Networks (RNNs),\\n'\n                   'describe the vanishing gradient problem, and implemen',\n        'score': 16.782752990722656},\n    {   'answer': 'Recurrent Neural Networks',\n        'context': 'rthermore, we will learn\\n'\n                   'how sequential modeling is related to Recurrent Neural '\n                   'Networks (RNN).We will\\n'\n                   'learn about the vanishing gradient problem in d',\n        'score': 16.642990112304688},\n    {   'answer': 'Recurrent Neural Networks',\n        'context': 'rthermore, we will learn\\n'\n                   'how sequential modeling is related to Recurrent Neural '\n                   'Networks (RNN). We will\\n'\n                   'learn about the vanishing gradient problem in ',\n        'score': 16.478988647460938},\n    {   'answer': 'Recurrent Neural Networks',\n        'context': 'rthermore, we will learn\\n'\n                   'how sequential modeling is related to Recurrent Neural '\n                   'Networks (RNN). We will\\n'\n                   'learn about the vanishing gradient problem in ',\n        'score': 16.478988647460938}]\n"
     ]
    }
   ],
   "source": [
    "print_answers(prediction, details=\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "12/02/2020 09:36:05 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.021s]\n",
      "12/02/2020 09:36:05 - INFO - haystack.finder -   Got 10 candidates from retriever\n",
      "12/02/2020 09:36:05 - INFO - haystack.finder -   Reader is looking for detailed answer in 152889 chars ...\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.89s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.26 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.25 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.15s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 2/2 [00:23<00:00, 11.55s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 2/2 [00:22<00:00, 11.42s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.00 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.07 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 2/2 [00:27<00:00, 13.97s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 2/2 [00:27<00:00, 13.95s/ Batches][   {   'answer': 'user-defined layer',\n",
      "        'context': 'tially, what we will do is\\n'\n",
      "                   'replace the last layer of VGG16 with a user-defined '\n",
      "                   'layer.Before we begin, ensure you have downloaded the '\n",
      "                   'image datasets fr',\n",
      "        'score': 10.711793899536133},\n",
      "    {   'answer': 'user-defined layer',\n",
      "        'context': 'tially, what we will do is\\n'\n",
      "                   'replace the last layer of VGG16 with a user-defined layer. '\n",
      "                   'Before we begin, ensure you have downloaded the image '\n",
      "                   'datasets f',\n",
      "        'score': 7.761836051940918},\n",
      "    {   'answer': 'user-defined layer',\n",
      "        'context': 'tially, what we will do is\\n'\n",
      "                   'replace the last layer of VGG16 with a user-defined layer. '\n",
      "                   'Before we begin, ensure you have downloaded the image '\n",
      "                   'datasets f',\n",
      "        'score': 7.761836051940918},\n",
      "    {   'answer': 'single hidden layer',\n",
      "        'context': 'will stick\\n'\n",
      "                   'to the simplest case, which is a neural network with a '\n",
      "                   'single hidden layer.You will learn\\n'\n",
      "                   'how to define a model in Keras, choose the hyperp',\n",
      "        'score': 7.02977180480957},\n",
      "    {   'answer': 'first\\nhidden layer',\n",
      "        'context': 'However, add a dropout regularization of rate=0.2 to the '\n",
      "                   'first\\n'\n",
      "                   'hidden layer and rate=0.1 to the remaining layers of your '\n",
      "                   'model and repeat the\\n'\n",
      "                   'compilat',\n",
      "        'score': 6.401031017303467}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_answers(finder.get_answers(question=\"What is a layer?\", top_k_retriever=10, top_k_reader=5), details=\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "12/02/2020 09:38:10 - INFO - elasticsearch -   POST http://localhost:9200/docuement/_delete_by_query [status:404 request:0.006s]\n",
      "12/02/2020 09:38:12 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:0.966s]\n"
     ]
    }
   ],
   "source": [
    "document_store.delete_all_documents(index=\"docuement\")\n",
    "document_store.write_documents(word)\n",
    "retriever = ElasticsearchRetriever(document_store=document_store)\n",
    "finder = Finder(farm_reader, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "12/02/2020 09:38:14 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.022s]\n",
      "12/02/2020 09:38:14 - INFO - haystack.finder -   Got 10 candidates from retriever\n",
      "12/02/2020 09:38:14 - INFO - haystack.finder -   Reader is looking for detailed answer in 83918 chars ...\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.59s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.22 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.06 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.20 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.18 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.20 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.17 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.19 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 2/2 [00:26<00:00, 13.20s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 2/2 [00:27<00:00, 13.82s/ Batches]\n"
     ]
    }
   ],
   "source": [
    "prediction = finder.get_answers(question=\"What is RNN?\", top_k_retriever=10, top_k_reader=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[   {   'answer': 'Recurrent Neural Networks',\n        'context': 'rthermore, we will learn\\n'\n                   'how sequential modeling is related to Recurrent Neural '\n                   'Networks (RNN).We will\\n'\n                   'learn about the vanishing gradient problem in d',\n        'score': 16.642990112304688},\n    {   'answer': 'Recurrent Neural Networks',\n        'context': 'rthermore, we will learn\\n'\n                   'how sequential modeling is related to Recurrent Neural '\n                   'Networks (RNN).We will\\n'\n                   'learn about the vanishing gradient problem in d',\n        'score': 16.642990112304688},\n    {   'answer': 'Recurrent Neural Networks',\n        'context': ' chapter, you will\\n'\n                   'be able to build sequential models, explain Recurrent '\n                   'Neural Networks (RNNs),\\n'\n                   'describe the vanishing gradient problem, and implemen',\n        'score': 15.48896312713623},\n    {   'answer': 'Recurrent Neural Networks',\n        'context': ' chapter, you will\\n'\n                   'be able to build sequential models, explain Recurrent '\n                   'Neural Networks (RNNs),\\n'\n                   'describe the vanishing gradient problem, and implemen',\n        'score': 15.48896312713623},\n    {   'answer': 'the hidden layer not only gives the\\n'\n                  'output, but it also feeds back the information of the '\n                  'output into itself',\n        'context': 'y of the RNN is that the hidden layer not only gives the\\n'\n                   'output, but it also feeds back the information of the '\n                   'output into itself. Before taking a\\n'\n                   'dee',\n        'score': 13.693136215209961}]\n"
     ]
    }
   ],
   "source": [
    "print_answers(prediction, details=\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}