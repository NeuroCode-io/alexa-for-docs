{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.1 64-bit ('.venv')",
   "metadata": {
    "interpreter": {
     "hash": "5bdca235408bc63495fd8719d073449338626e85bb6ce08bacdb45b63101d775"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.file_converter.pdf import PDFToTextConverter\n",
    "converter = PDFToTextConverter(remove_numeric_tables=True, valid_languages=[\"en\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "book = converter.convert(file_path=\"/home/elena/Downloads/data/9781839217579-THE_DEEP_LEARNING_WITH_KERAS_WORKSHOP_SECOND_EDITION.pdf\", meta=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(dict, 2, dict_keys(['text', 'meta']), 579763, NoneType)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "type(book), len(book), book.keys(), len(book[\"text\"]), type(book[\"meta\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.preprocessor.preprocessor import PreProcessor\n",
    "\n",
    "# we can use PreProcessor to split by passage, sentence and word\n",
    "# passage wil split data, i.e. book in to dicts; one with meta _split_id 0 and the other _split_id 1 \n",
    "# sentence with split length 260 will split book in 14 dicts; key meta numerating those dicts (from 0 to 13)\n",
    "# word with split length 260 will split book into 329 dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /home/elena/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "processor_word = PreProcessor(clean_empty_lines=True,\n",
    "                         clean_whitespace=True,\n",
    "                         clean_header_footer=True,\n",
    "                         split_by=\"word\",\n",
    "                         split_length = 260,\n",
    "                         split_respect_sentence_boundary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A sentence found with word count higher than the split length.\n"
     ]
    }
   ],
   "source": [
    "word=processor_word.process(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(list, 329)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "type(word), len(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "12/02/2020 11:03:12 - INFO - faiss -   Loading faiss with AVX2 support.\n",
      "12/02/2020 11:03:12 - INFO - faiss -   Loading faiss.\n",
      "12/02/2020 11:03:13 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n",
      "12/02/2020 11:03:13 - INFO - farm.infer -   Could not find `deepset/roberta-base-squad2` locally. Try to download from model hub ...\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at deepset/roberta-base-squad2 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "12/02/2020 11:03:17 - WARNING - farm.modeling.language_model -   Could not automatically detect from language model name what language it is. \n",
      "\t We guess it's an *ENGLISH* model ... \n",
      "\t If not: Init the language model by supplying the 'language' param.\n",
      "12/02/2020 11:03:25 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n",
      "12/02/2020 11:03:25 - INFO - farm.infer -   Got ya 3 parallel workers to do inference ...\n",
      "12/02/2020 11:03:25 - INFO - farm.infer -    0    0    0 \n",
      "12/02/2020 11:03:25 - INFO - farm.infer -   /w\\  /w\\  /w\\\n",
      "12/02/2020 11:03:25 - INFO - farm.infer -   /'\\  / \\  /'\\\n",
      "12/02/2020 11:03:25 - INFO - farm.infer -       \n"
     ]
    }
   ],
   "source": [
    "from haystack.reader.farm import FARMReader\n",
    "farm_reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "12/02/2020 11:03:31 - INFO - elasticsearch -   HEAD http://localhost:9200/document [status:200 request:0.008s]\n",
      "12/02/2020 11:03:31 - INFO - elasticsearch -   GET http://localhost:9200/document [status:200 request:0.005s]\n",
      "12/02/2020 11:03:31 - INFO - elasticsearch -   PUT http://localhost:9200/document/_mapping [status:200 request:0.015s]\n",
      "12/02/2020 11:03:31 - INFO - elasticsearch -   HEAD http://localhost:9200/label [status:200 request:0.003s]\n"
     ]
    }
   ],
   "source": [
    "from haystack.document_store.elasticsearch import ElasticsearchDocumentStore\n",
    "document_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\", index=\"document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "12/02/2020 11:04:07 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:0.826s]\n"
     ]
    }
   ],
   "source": [
    "document_store.write_documents(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.retriever.sparse import ElasticsearchRetriever\n",
    "retriever = ElasticsearchRetriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder = Finder(farm_reader, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "12/02/2020 11:04:59 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.012s]\n",
      "12/02/2020 11:04:59 - INFO - haystack.finder -   Got 1 candidates from retriever\n",
      "12/02/2020 11:04:59 - INFO - haystack.finder -   Reader is looking for detailed answer in 1459 chars ...\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.70s/ Batches]\n"
     ]
    }
   ],
   "source": [
    "prediction = finder.get_answers(question=\"layer\", top_k_retriever=1, top_k_reader=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.utils import print_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[   {   'answer': 'layer 1',\n        'context': 'se, W2 has\\n'\n                   'three rows and three columns because the input to layer 2 '\n                   'is layer 1, which has two\\n'\n                   'nodes, and layer 2 has five nodes.The bias, however, is',\n        'score': 1.060227632522583},\n    {   'answer': '2',\n        'context': 'tivation of layer 1.The output\\n'\n                   'of layer 1 is, in fact, the input for layer 2.Next, the '\n                   'activation of layer 1 is the matrix\\n'\n                   'multiplied by the weight ma',\n        'score': -2.83345890045166}]\n"
     ]
    }
   ],
   "source": [
    "print_answers(prediction, details=\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "12/02/2020 11:05:45 - INFO - haystack.preprocessor.utils -   Converting /home/elena/Downloads/data/9781839217579-THE_DEEP_LEARNING_WITH_KERAS_WORKSHOP_SECOND_EDITION.pdf\n"
     ]
    }
   ],
   "source": [
    "from haystack.preprocessor.utils import convert_files_to_dicts\n",
    "dicts=convert_files_to_dicts(\"/home/elena/Downloads/data/\", split_paragraphs=True) # no cleaning function applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "12/02/2020 11:06:08 - INFO - elasticsearch -   POST http://localhost:9200/document/_delete_by_query [status:200 request:0.328s]\n"
     ]
    }
   ],
   "source": [
    "document_store.delete_all_documents(index=\"document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "12/02/2020 11:06:19 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:0.981s]\n",
      "12/02/2020 11:06:20 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.112s]\n",
      "12/02/2020 11:06:21 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.085s]\n"
     ]
    }
   ],
   "source": [
    "document_store.write_documents(dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = ElasticsearchRetriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder = Finder(farm_reader, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "12/02/2020 11:07:15 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.008s]\n",
      "12/02/2020 11:07:15 - INFO - haystack.finder -   Got 10 candidates from retriever\n",
      "12/02/2020 11:07:15 - INFO - haystack.finder -   Reader is looking for detailed answer in 7625 chars ...\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.10 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.44 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.42 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.46 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.84 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.30 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.10s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.26 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.48 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.79 Batches/s]\n"
     ]
    }
   ],
   "source": [
    "prediction = finder.get_answers(question=\"layer\", top_k_retriever=10, top_k_reader=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[   {   'answer': 'two-layer',\n        'context': 'ample.\\n'\n                   \"Let's go through forward propagation equations one by one \"\n                   'for a two-layer\\n'\n                   'neural network (shown in the following image) where the '\n                   'input data is',\n        'score': 1.6360710859298706},\n    {   'answer': '1',\n        'context': '2. Next, the layer 1 output is computed by applying an '\n                   'activation function to z1, which\\n'\n                   'is the output of the previous step:\\n'\n                   'a1 = tanh(z1)\\n'\n                   '3. a1 is the',\n        'score': 0.4360466003417969},\n    {   'answer': 'four',\n        'context': '2. Import all the necessary dependencies. Build a '\n                   'four-layer Keras sequential model\\n'\n                   'without dropout regularization. Build the network with 16 '\n                   'units in',\n        'score': 0.2117246687412262},\n    {   'answer': 'two hidden layers',\n        'context': 'allow\\n'\n                   'neural network, whereas the neural network in Figure 3.3 '\n                   'has two hidden layers, so it is\\n'\n                   'a deep neural network. The input layers are generally o',\n        'score': -0.7472821474075317},\n    {   'answer': 'convolution layer',\n        'context': 'Convolution Layer\\n'\n                   'The convolution layer is the place where image processing '\n                   'starts. A convolution\\n'\n                   'layer consists of two parts:\\n'\n                   '• Feature detector or f',\n        'score': -2.2045319080352783}]\n"
     ]
    }
   ],
   "source": [
    "print_answers(prediction, details=\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}