{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime import InferenceSession\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from pathlib import Path\n",
    "from transformers.convert_graph_to_onnx import convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_name=\"./onnx_model/roberta-base-squad2.onnx\"\n",
    "model_name=\"deepset/roberta-base-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_for_onnx(model_path, onnx_model_name, tokenizer):\n",
    "    if Path(onnx_model_name).exists():\n",
    "        print(\"ONNX input exists\")\n",
    "        return\n",
    "    convert(\n",
    "        framework=\"pt\",\n",
    "        model=model_path,\n",
    "        tokenizer=tokenizer,\n",
    "        output=Path(onnx_model_name),\n",
    "        pipeline_name=\"question-answering\",\n",
    "        opset=12\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ONNX input exists\n"
     ]
    }
   ],
   "source": [
    "convert_for_onnx(model_path=model, onnx_model_name=onnx_model_name, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need example.context_text and example.question_text\n",
    "examples={\"context_text\": \"In its early years, the new convention center failed to meet attendance and revenue expectations.[12] By 2002, many Silicon Valley businesses were choosing the much larger Moscone Center in San Francisco over the San Jose Convention Center due to the latter's limited space. A ballot measure to finance an expansion via a hotel tax failed to reach the required two-thirds majority to pass. In June 2005, Team San Jose built the South Hall, a $6.77 million, blue and white tent, adding 80,000 square feet (7,400 m2) of exhibit space\", \"question_text\": \"where is the businesses choosing to go?\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding=\"longest\"\n",
    "max_seq_len=384\n",
    "doc_stride=128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'where is the businesses choosing to go?'"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "examples[\"question_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = []\n",
    "for example in examples:\n",
    "    # Define the side we want to truncate / pad and the text/pair sorting\n",
    "    question_first = bool(tokenizer.padding_side == \"right\")\n",
    "    encoded_inputs = tokenizer(\n",
    "        text=example.question_text if question_first else example.context_text,\n",
    "        text_pair=example[\"context_text\"] if question_first else example[\"question_text\"],\n",
    "        padding=padding,\n",
    "        truncation=\"only_second\" if question_first else \"only_first\",\n",
    "        max_length=max_seq_len,\n",
    "        stride=doc_stride,\n",
    "        return_tensors=\"np\",\n",
    "        return_token_type_ids=True,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        return_special_tokens_mask=True,\n",
    "        )\n",
    "    \n",
    "    # When the input is too long, it's converted in a batch of inputs with overflowing tokens\n",
    "    # and a stride of overlap between the inputs. If a batch of inputs is given, a special output\n",
    "    # \"overflow_to_sample_mapping\" indicate which member of the encoded batch belong to which original batch sample.\n",
    "    # Here we tokenize examples one-by-one so we don't need to use \"overflow_to_sample_mapping\".\n",
    "    # \"num_span\" is the number of output samples generated from the overflowing tokens.\n",
    "    num_spans = len(encoded_inputs[\"input_ids\"])\n",
    "\n",
    "    # p_mask: mask with 1 for token than cannot be in the answer (0 for token which can be in an answer)\n",
    "    # We put 0 on the tokens from the context and 1 everywhere else (question and special tokens)\n",
    "    p_mask = np.asarray(\n",
    "                    [\n",
    "                        [tok != 1 if question_first else 0 for tok in encoded_inputs.sequence_ids(span_id)]\n",
    "                        for span_id in range(num_spans)\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                # keep the cls_token unmasked (some models use it to indicate unanswerable questions)\n",
    "                if self.tokenizer.cls_token_id:\n",
    "                    cls_index = np.nonzero(encoded_inputs[\"input_ids\"] == self.tokenizer.cls_token_id)\n",
    "                    p_mask[cls_index] = 0\n",
    "\n",
    "                features = []\n",
    "                for span_idx in range(num_spans):\n",
    "                    features.append(\n",
    "                        SquadFeatures(\n",
    "                            input_ids=encoded_inputs[\"input_ids\"][span_idx],\n",
    "                            attention_mask=encoded_inputs[\"attention_mask\"][span_idx],\n",
    "                            token_type_ids=encoded_inputs[\"token_type_ids\"][span_idx],\n",
    "                            p_mask=p_mask[span_idx].tolist(),\n",
    "                            encoding=encoded_inputs[span_idx],\n",
    "                            # We don't use the rest of the values - and actually\n",
    "                            # for Fast tokenizer we could totally avoid using SquadFeatures and SquadExample\n",
    "                            cls_index=None,\n",
    "                            token_to_orig_map={},\n",
    "                            example_index=0,\n",
    "                            unique_id=0,\n",
    "                            paragraph_len=0,\n",
    "                            token_is_max_context=0,\n",
    "                            tokens=[],\n",
    "                            start_position=0,\n",
    "                            end_position=0,\n",
    "                            is_impossible=False,\n",
    "                            qas_id=None,\n",
    "                        )\n",
    "                    )\n",
    "                features_list.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.data.processors.squad import squad_convert_example_to_features\n",
    "\n",
    "examples = [{'context_text': \"In its early years, the new convention center failed to meet attendance and revenue expectations.[12] By 2002, many Silicon Valley businesses were choosing the much larger Moscone Center in San Francisco over the San Jose Convention Center due to the latter's limited space. A ballot measure to finance an expansion via a hotel tax failed to reach the required two-thirds majority to pass. In June 2005, Team San Jose built the South Hall, a $6.77 million, blue and white tent, adding 80,000 square feet (7,400 m2) of exhibit space\",\n",
    "'question_text': \"where is the businesses choosing to go?\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.tokenization_utils_base import PaddingStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "squad_convert_example_to_features() got an unexpected keyword argument 'examples'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-bc313f13a982>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msquad_convert_example_to_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m384\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_stride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_query_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: squad_convert_example_to_features() got an unexpected keyword argument 'examples'"
     ]
    }
   ],
   "source": [
    "squad_convert_example_to_features(examples=[examples], tokenizer=tokenizer, max_seq_length=384, doc_stride=128, max_query_length=64, is_training=False, return_dataset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "squad_convert_example_to_features() got an unexpected keyword argument 'examples'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-715b8bfc9e64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m features_list=squad_convert_example_to_features(examples=[examples],\n\u001b[0m\u001b[1;32m      2\u001b[0m         \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m384\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mdoc_stride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mmax_query_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: squad_convert_example_to_features() got an unexpected keyword argument 'examples'"
     ]
    }
   ],
   "source": [
    "features_list=squad_convert_example_to_features(examples=[examples],\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=384,\n",
    "        doc_stride=128,\n",
    "        max_query_length=64,\n",
    "        padding_strategy=PaddingStrategy.MAX_LENGTH.value,\n",
    "        is_training=False,\n",
    "        tqdm_enabled=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"deepset/roberta-base-squad2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "from transformers import pipeline\n",
    "nlp=pipeline(task=\"question-answering\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'ArgumentHandler' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-06fafd4aa376>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mArgumentHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mABS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ArgumentHandler' is not defined"
     ]
    }
   ],
   "source": [
    "ArgumentHandler(ABS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-fef7eef61126>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransformers_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"context\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"context_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"question\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformers_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m384\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_stride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "transformers_query = {\"context\": examples[\"context_text\"], \"question\": examples[\"question_text\"]}\n",
    "predictions = nlp(transformers_query, topk=1, max_seq_len=384, doc_stride=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = InferenceSession(onnx_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NodeArg(name='input_ids', type='tensor(int64)', shape=['batch', 'sequence'])\nNodeArg(name='attention_mask', type='tensor(int64)', shape=['batch', 'sequence'])\n"
     ]
    }
   ],
   "source": [
    "for input_meta in session.get_inputs():\n",
    "    print(input_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "context=\"In its early years, the new convention center failed to meet attendance and revenue expectations.[12] By 2002, many Silicon Valley businesses were choosing the much larger Moscone Center in San Francisco over the San Jose Convention Center due to the latter's limited space. A ballot measure to finance an expansion via a hotel tax failed to reach the required two-thirds majority to pass. In June 2005, Team San Jose built the South Hall, a $6.77 million, blue and white tent, adding 80,000 square feet (7,400 m2) of exhibit space\"\n",
    "tokens_context = tokenizer(context, return_attention_mask=True, return_tensors=\"np\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"where is the businesses choosing to go?\"\n",
    "tokens_question = tokenizer(question, return_attention_mask=True, return_tensors=\"np\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': array([[   0, 8569,   16,    5, 1252, 8348,    7,  213,  116,    2]]),\n",
       " 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "tokens_question.__dict__[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': array([[    0,  1121,    63,   419,   107,     6,     5,    92,  8825,\n",
       "          1312,  1447,     7,   972,  6856,     8,   903,  2113, 31274,\n",
       "          1092,   742,   870,  5241,     6,   171, 10087,  1739,  1252,\n",
       "            58,  8348,     5,   203,  2514,  8033, 33666,   824,    11,\n",
       "           764,  2659,    81,     5,   764,  3071,  9127,   824,   528,\n",
       "             7,     5,  5442,    18,  1804,   980,     4,    83,  5250,\n",
       "          2450,     7,  2879,    41,  2919,  1241,    10,  2303,   629,\n",
       "          1447,     7,  1338,     5,  1552,    80,    12, 10224,  1647,\n",
       "             7,  1323,     4,    96,   502,  4013,     6,  2711,   764,\n",
       "          3071,  1490,     5,   391,  1631,     6,    10,    68,   401,\n",
       "             4,  4718,   153,     6,  2440,     8,  1104, 10178,     6,\n",
       "          1271,  1812,     6,   151,  3925,  1730,    36,   406,     6,\n",
       "          4017,   475,   176,    43,     9,  8483,   980,     2]]),\n",
       " 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "tokens_context.__dict__[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}