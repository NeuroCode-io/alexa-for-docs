{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.1 64-bit ('.venv')",
   "metadata": {
    "interpreter": {
     "hash": "4037c546effa33586e051666d7d74862170a63a04006db20983879415d1a70a6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "In Transformers v4.0.0, the default path to cache downloaded models changed from '~/.cache/torch/transformers' to '~/.cache/huggingface/transformers'. Since you don't seem to have overridden and '~/.cache/torch/transformers' is a directory that exists, we're moving it to '~/.cache/huggingface/transformers' to avoid redownloading models you have already in the cache. You should only see this message once.\n"
     ]
    }
   ],
   "source": [
    "from onnxruntime import InferenceSession\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from pathlib import Path\n",
    "from transformers.convert_graph_to_onnx import convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|██████████| 571/571 [00:00<00:00, 289kB/s]\n",
      "Downloading: 100%|██████████| 899k/899k [00:00<00:00, 1.44MB/s]\n",
      "Downloading: 100%|██████████| 456k/456k [00:00<00:00, 873kB/s]\n",
      "Downloading: 100%|██████████| 772/772 [00:00<00:00, 327kB/s]\n",
      "Downloading: 100%|██████████| 79.0/79.0 [00:00<00:00, 17.9kB/s]\n"
     ]
    }
   ],
   "source": [
    "onnx_model_name=\"./onnx_model/roberta-base-squad2.onnx\"\n",
    "model_name=\"deepset/roberta-base-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|██████████| 496M/496M [01:37<00:00, 5.10MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_for_onnx(model_path, onnx_model_name, tokenizer):\n",
    "    if Path(onnx_model_name).exists():\n",
    "        print(\"ONNX input exists\")\n",
    "        return\n",
    "    convert(\n",
    "        framework=\"pt\",\n",
    "        model=model_path,\n",
    "        tokenizer=tokenizer,\n",
    "        output=Path(onnx_model_name),\n",
    "        pipeline_name=\"question-answering\",\n",
    "        opset=12\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ONNX opset version set to: 12\n",
      "Loading pipeline (model: RobertaForQuestionAnswering(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
      "), tokenizer: PreTrainedTokenizerFast(name_or_path='deepset/roberta-base-squad2', vocab_size=50265, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True)}))\n",
      "Using framework PyTorch: 1.7.1\n",
      "Found input input_ids with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found input attention_mask with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found output output_0 with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found output output_1 with shape: {0: 'batch', 1: 'sequence'}\n",
      "Ensuring inputs are in correct order\n",
      "token_type_ids is not present in the generated input list.\n",
      "Generated inputs order: ['input_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "convert_for_onnx(model_path=model, onnx_model_name=onnx_model_name, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need example.context_text and example.question_text\n",
    "examples=[{\"context_text\": \"In its early years, the new convention center failed to meet attendance and revenue expectations.[12] By 2002, many Silicon Valley businesses were choosing the much larger Moscone Center in San Francisco over the San Jose Convention Center due to the latter's limited space. A ballot measure to finance an expansion via a hotel tax failed to reach the required two-thirds majority to pass. In June 2005, Team San Jose built the South Hall, a $6.77 million, blue and white tent, adding 80,000 square feet (7,400 m2) of exhibit space\", \"question_text\": \"where is the businesses choosing to go?\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding=\"longest\"\n",
    "max_seq_len=384\n",
    "doc_stride=128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "where is the businesses choosing to go?\n"
     ]
    }
   ],
   "source": [
    "for example in examples:\n",
    "    print(example[\"question_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'where is the businesses choosing to go?'"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "examples[0][\"question_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadExample(object):\n",
    "    \"\"\"\n",
    "    A single training/test example for the Squad dataset, as loaded from disk.\n",
    "\n",
    "    Args:\n",
    "        qas_id: The example's unique identifier\n",
    "        question_text: The question string\n",
    "        context_text: The context string\n",
    "        answer_text: The answer string\n",
    "        start_position_character: The character position of the start of the answer\n",
    "        title: The title of the example\n",
    "        answers: None by default, this is used during evaluation. Holds answers as well as their start positions.\n",
    "        is_impossible: False by default, set to True if the example has no possible answer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        qas_id,\n",
    "        question_text,\n",
    "        context_text,\n",
    "        answer_text,\n",
    "        start_position_character,\n",
    "        title,\n",
    "        answers=[],\n",
    "        is_impossible=False,\n",
    "    ):\n",
    "        self.qas_id = qas_id\n",
    "        self.question_text = question_text\n",
    "        self.context_text = context_text\n",
    "        self.answer_text = answer_text\n",
    "        self.title = title\n",
    "        self.is_impossible = is_impossible\n",
    "        self.answers = answers\n",
    "\n",
    "        self.start_position, self.end_position = 0, 0\n",
    "\n",
    "        doc_tokens = []\n",
    "        char_to_word_offset = []\n",
    "        prev_is_whitespace = True\n",
    "\n",
    "        # Split on whitespace so that different tokens may be attributed to their original position.\n",
    "        for c in self.context_text:\n",
    "            if _is_whitespace(c):\n",
    "                prev_is_whitespace = True\n",
    "            else:\n",
    "                if prev_is_whitespace:\n",
    "                    doc_tokens.append(c)\n",
    "                else:\n",
    "                    doc_tokens[-1] += c\n",
    "                prev_is_whitespace = False\n",
    "            char_to_word_offset.append(len(doc_tokens) - 1)\n",
    "\n",
    "        self.doc_tokens = doc_tokens\n",
    "        self.char_to_word_offset = char_to_word_offset\n",
    "\n",
    "        # Start end end positions only has a value during evaluation.\n",
    "        if start_position_character is not None and not is_impossible:\n",
    "            self.start_position = char_to_word_offset[start_position_character]\n",
    "            self.end_position = char_to_word_offset[\n",
    "                min(start_position_character + len(answer_text) - 1, len(char_to_word_offset) - 1)\n",
    "            ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadFeatures(object):\n",
    "    \"\"\"\n",
    "    Single squad example features to be fed to a model.\n",
    "    Those features are model-specific and can be crafted from :class:`~transformers.data.processors.squad.SquadExample`\n",
    "    using the :method:`~transformers.data.processors.squad.squad_convert_examples_to_features` method.\n",
    "\n",
    "    Args:\n",
    "        input_ids: Indices of input sequence tokens in the vocabulary.\n",
    "        attention_mask: Mask to avoid performing attention on padding token indices.\n",
    "        token_type_ids: Segment token indices to indicate first and second portions of the inputs.\n",
    "        cls_index: the index of the CLS token.\n",
    "        p_mask: Mask identifying tokens that can be answers vs. tokens that cannot.\n",
    "            Mask with 1 for tokens than cannot be in the answer and 0 for token that can be in an answer\n",
    "        example_index: the index of the example\n",
    "        unique_id: The unique Feature identifier\n",
    "        paragraph_len: The length of the context\n",
    "        token_is_max_context: List of booleans identifying which tokens have their maximum context in this feature object.\n",
    "            If a token does not have their maximum context in this feature object, it means that another feature object\n",
    "            has more information related to that token and should be prioritized over this feature for that token.\n",
    "        tokens: list of tokens corresponding to the input ids\n",
    "        token_to_orig_map: mapping between the tokens and the original text, needed in order to identify the answer.\n",
    "        start_position: start of the answer token index \n",
    "        end_position: end of the answer token index \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask,\n",
    "        token_type_ids,\n",
    "        cls_index,\n",
    "        p_mask,\n",
    "        example_index,\n",
    "        unique_id,\n",
    "        paragraph_len,\n",
    "        token_is_max_context,\n",
    "        tokens,\n",
    "        token_to_orig_map,\n",
    "        start_position,\n",
    "        end_position,\n",
    "    ):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.token_type_ids = token_type_ids\n",
    "        self.cls_index = cls_index\n",
    "        self.p_mask = p_mask\n",
    "\n",
    "        self.example_index = example_index\n",
    "        self.unique_id = unique_id\n",
    "        self.paragraph_len = paragraph_len\n",
    "        self.token_is_max_context = token_is_max_context\n",
    "        self.tokens = tokens\n",
    "        self.token_to_orig_map = token_to_orig_map\n",
    "\n",
    "        self.start_position = start_position\n",
    "        self.end_position = end_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squad_convert_examples_to_features(\n",
    "    examples, tokenizer, max_seq_length, doc_stride, max_query_length, is_training, return_dataset=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Converts a list of examples into a list of features that can be directly given as input to a model.\n",
    "    It is model-dependant and takes advantage of many of the tokenizer's features to create the model's inputs.\n",
    "\n",
    "    Args:\n",
    "        examples: list of :class:`~transformers.data.processors.squad.SquadExample`\n",
    "        tokenizer: an instance of a child of :class:`~transformers.PreTrainedTokenizer`\n",
    "        max_seq_length: The maximum sequence length of the inputs.\n",
    "        doc_stride: The stride used when the context is too large and is split across several features.\n",
    "        max_query_length: The maximum length of the query.\n",
    "        is_training: whether to create features for model evaluation or model training.\n",
    "        return_dataset: Default False. Either 'pt' or 'tf'.\n",
    "            if 'pt': returns a torch.data.TensorDataset,\n",
    "            if 'tf': returns a tf.data.Dataset\n",
    "\n",
    "    Returns:\n",
    "        list of :class:`~transformers.data.processors.squad.SquadFeatures`\n",
    "\n",
    "    Example::\n",
    "\n",
    "        processor = SquadV2Processor()\n",
    "        examples = processor.get_dev_examples(data_dir)\n",
    "\n",
    "        features = squad_convert_examples_to_features( \n",
    "            examples=examples,\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_length=args.max_seq_length,\n",
    "            doc_stride=args.doc_stride,\n",
    "            max_query_length=args.max_query_length,\n",
    "            is_training=not evaluate,\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    # Defining helper methods\n",
    "    unique_id = 1000000000\n",
    "\n",
    "    features = []\n",
    "    for (example_index, example) in enumerate(tqdm(examples, desc=\"Converting examples to features\")):\n",
    "        if is_training and not example.is_impossible:\n",
    "            # Get start and end position\n",
    "            start_position = example.start_position\n",
    "            end_position = example.end_position\n",
    "\n",
    "            # If the answer cannot be found in the text, then skip this example.\n",
    "            actual_text = \" \".join(example.doc_tokens[start_position : (end_position + 1)])\n",
    "            cleaned_answer_text = \" \".join(whitespace_tokenize(example.answer_text))\n",
    "            if actual_text.find(cleaned_answer_text) == -1:\n",
    "                logger.warning(\"Could not find answer: '%s' vs. '%s'\", actual_text, cleaned_answer_text)\n",
    "                continue\n",
    "\n",
    "        tok_to_orig_index = []\n",
    "        orig_to_tok_index = []\n",
    "        all_doc_tokens = []\n",
    "        for (i, token) in enumerate(example.doc_tokens):\n",
    "            orig_to_tok_index.append(len(all_doc_tokens))\n",
    "            sub_tokens = tokenizer.tokenize(token)\n",
    "            for sub_token in sub_tokens:\n",
    "                tok_to_orig_index.append(i)\n",
    "                all_doc_tokens.append(sub_token)\n",
    "\n",
    "        if is_training and not example.is_impossible:\n",
    "            tok_start_position = orig_to_tok_index[example.start_position]\n",
    "            if example.end_position < len(example.doc_tokens) - 1:\n",
    "                tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n",
    "            else:\n",
    "                tok_end_position = len(all_doc_tokens) - 1\n",
    "\n",
    "            (tok_start_position, tok_end_position) = _improve_answer_span(\n",
    "                all_doc_tokens, tok_start_position, tok_end_position, tokenizer, example.answer_text\n",
    "            )\n",
    "\n",
    "        spans = []\n",
    "\n",
    "        truncated_query = tokenizer.encode(\n",
    "            example.question_text, add_special_tokens=False, max_length=max_query_length\n",
    "        )\n",
    "        sequence_added_tokens = tokenizer.max_len - tokenizer.max_len_single_sentence\n",
    "        sequence_pair_added_tokens = tokenizer.max_len - tokenizer.max_len_sentences_pair\n",
    "\n",
    "        span_doc_tokens = all_doc_tokens\n",
    "        while len(spans) * doc_stride < len(all_doc_tokens):\n",
    "\n",
    "            encoded_dict = tokenizer.encode_plus(\n",
    "                truncated_query if tokenizer.padding_side == \"right\" else span_doc_tokens,\n",
    "                span_doc_tokens if tokenizer.padding_side == \"right\" else truncated_query,\n",
    "                max_length=max_seq_length,\n",
    "                return_overflowing_tokens=True,\n",
    "                pad_to_max_length=True,\n",
    "                stride=max_seq_length - doc_stride - len(truncated_query) - sequence_pair_added_tokens,\n",
    "                truncation_strategy=\"only_second\" if tokenizer.padding_side == \"right\" else \"only_first\",\n",
    "            )\n",
    "\n",
    "            paragraph_len = min(\n",
    "                len(all_doc_tokens) - len(spans) * doc_stride,\n",
    "                max_seq_length - len(truncated_query) - sequence_pair_added_tokens,\n",
    "            )\n",
    "\n",
    "            if tokenizer.pad_token_id in encoded_dict[\"input_ids\"]:\n",
    "                non_padded_ids = encoded_dict[\"input_ids\"][: encoded_dict[\"input_ids\"].index(tokenizer.pad_token_id)]\n",
    "            else:\n",
    "                non_padded_ids = encoded_dict[\"input_ids\"]\n",
    "\n",
    "            tokens = tokenizer.convert_ids_to_tokens(non_padded_ids)\n",
    "\n",
    "            token_to_orig_map = {}\n",
    "            for i in range(paragraph_len):\n",
    "                index = len(truncated_query) + sequence_added_tokens + i if tokenizer.padding_side == \"right\" else i\n",
    "                token_to_orig_map[index] = tok_to_orig_index[len(spans) * doc_stride + i]\n",
    "\n",
    "            encoded_dict[\"paragraph_len\"] = paragraph_len\n",
    "            encoded_dict[\"tokens\"] = tokens\n",
    "            encoded_dict[\"token_to_orig_map\"] = token_to_orig_map\n",
    "            encoded_dict[\"truncated_query_with_special_tokens_length\"] = len(truncated_query) + sequence_added_tokens\n",
    "            encoded_dict[\"token_is_max_context\"] = {}\n",
    "            encoded_dict[\"start\"] = len(spans) * doc_stride\n",
    "            encoded_dict[\"length\"] = paragraph_len\n",
    "\n",
    "            spans.append(encoded_dict)\n",
    "\n",
    "            if \"overflowing_tokens\" not in encoded_dict:\n",
    "                break\n",
    "            span_doc_tokens = encoded_dict[\"overflowing_tokens\"]\n",
    "\n",
    "        for doc_span_index in range(len(spans)):\n",
    "            for j in range(spans[doc_span_index][\"paragraph_len\"]):\n",
    "                is_max_context = _new_check_is_max_context(spans, doc_span_index, doc_span_index * doc_stride + j)\n",
    "                index = (\n",
    "                    j\n",
    "                    if tokenizer.padding_side == \"left\"\n",
    "                    else spans[doc_span_index][\"truncated_query_with_special_tokens_length\"] + j\n",
    "                )\n",
    "                spans[doc_span_index][\"token_is_max_context\"][index] = is_max_context\n",
    "\n",
    "        for span in spans:\n",
    "            # Identify the position of the CLS token\n",
    "            cls_index = span[\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "\n",
    "            # p_mask: mask with 1 for token than cannot be in the answer (0 for token which can be in an answer)\n",
    "            # Original TF implem also keep the classification token (set to 0) (not sure why...)\n",
    "            p_mask = np.array(span[\"token_type_ids\"])\n",
    "\n",
    "            p_mask = np.minimum(p_mask, 1)\n",
    "\n",
    "            if tokenizer.padding_side == \"right\":\n",
    "                # Limit positive values to one\n",
    "                p_mask = 1 - p_mask\n",
    "\n",
    "            p_mask[np.where(np.array(span[\"input_ids\"]) == tokenizer.sep_token_id)[0]] = 1\n",
    "\n",
    "            # Set the CLS index to '0'\n",
    "            p_mask[cls_index] = 0\n",
    "\n",
    "            span_is_impossible = example.is_impossible\n",
    "            start_position = 0\n",
    "            end_position = 0\n",
    "            if is_training and not span_is_impossible:\n",
    "                # For training, if our document chunk does not contain an annotation\n",
    "                # we throw it out, since there is nothing to predict.\n",
    "                doc_start = span[\"start\"]\n",
    "                doc_end = span[\"start\"] + span[\"length\"] - 1\n",
    "                out_of_span = False\n",
    "\n",
    "                if not (tok_start_position >= doc_start and tok_end_position <= doc_end):\n",
    "                    out_of_span = True\n",
    "\n",
    "                if out_of_span:\n",
    "                    start_position = cls_index\n",
    "                    end_position = cls_index\n",
    "                    span_is_impossible = True\n",
    "                else:\n",
    "                    if tokenizer.padding_side == \"left\":\n",
    "                        doc_offset = 0\n",
    "                    else:\n",
    "                        doc_offset = len(truncated_query) + sequence_added_tokens\n",
    "\n",
    "                    start_position = tok_start_position - doc_start + doc_offset\n",
    "                    end_position = tok_end_position - doc_start + doc_offset\n",
    "\n",
    "            features.append(\n",
    "                SquadFeatures(\n",
    "                    span[\"input_ids\"],\n",
    "                    span[\"attention_mask\"],\n",
    "                    span[\"token_type_ids\"],\n",
    "                    cls_index,\n",
    "                    p_mask.tolist(),\n",
    "                    example_index=example_index,\n",
    "                    unique_id=unique_id,\n",
    "                    paragraph_len=span[\"paragraph_len\"],\n",
    "                    token_is_max_context=span[\"token_is_max_context\"],\n",
    "                    tokens=span[\"tokens\"],\n",
    "                    token_to_orig_map=span[\"token_to_orig_map\"],\n",
    "                    start_position=start_position,\n",
    "                    end_position=end_position,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            unique_id += 1\n",
    "\n",
    "    if return_dataset == \"pt\":\n",
    "        if not is_torch_available():\n",
    "            raise ImportError(\"Pytorch must be installed to return a pytorch dataset.\")\n",
    "\n",
    "        # Convert to Tensors and build dataset\n",
    "        all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "        all_attention_masks = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
    "        all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n",
    "        all_cls_index = torch.tensor([f.cls_index for f in features], dtype=torch.long)\n",
    "        all_p_mask = torch.tensor([f.p_mask for f in features], dtype=torch.float)\n",
    "\n",
    "        if not is_training:\n",
    "            all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n",
    "            dataset = TensorDataset(\n",
    "                all_input_ids, all_attention_masks, all_token_type_ids, all_example_index, all_cls_index, all_p_mask\n",
    "            )\n",
    "        else:\n",
    "            all_start_positions = torch.tensor([f.start_position for f in features], dtype=torch.long)\n",
    "            all_end_positions = torch.tensor([f.end_position for f in features], dtype=torch.long)\n",
    "            dataset = TensorDataset(\n",
    "                all_input_ids,\n",
    "                all_attention_masks,\n",
    "                all_token_type_ids,\n",
    "                all_start_positions,\n",
    "                all_end_positions,\n",
    "                all_cls_index,\n",
    "                all_p_mask,\n",
    "            )\n",
    "\n",
    "        return features, dataset\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgumentHandler(ABC):\n",
    "    \"\"\"\n",
    "    Base interface for handling arguments for each :class:`~transformers.pipelines.Pipeline`.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionAnsweringArgumentHandler(ArgumentHandler):\n",
    "    \"\"\"\n",
    "    QuestionAnsweringPipeline requires the user to provide multiple arguments (i.e. question & context) to be mapped to\n",
    "    internal :class:`~transformers.SquadExample`.\n",
    "\n",
    "    QuestionAnsweringArgumentHandler manages all the possible to create a :class:`~transformers.SquadExample` from the\n",
    "    command-line supplied arguments.\n",
    "    \"\"\"\n",
    "\n",
    "    def normalize(self, item):\n",
    "        if isinstance(item, SquadExample):\n",
    "            return item\n",
    "        elif isinstance(item, dict):\n",
    "            for k in [\"question\", \"context\"]:\n",
    "                if k not in item:\n",
    "                    raise KeyError(\"You need to provide a dictionary with keys {question:..., context:...}\")\n",
    "                elif item[k] is None:\n",
    "                    raise ValueError(\"`{}` cannot be None\".format(k))\n",
    "                elif isinstance(item[k], str) and len(item[k]) == 0:\n",
    "                    raise ValueError(\"`{}` cannot be empty\".format(k))\n",
    "\n",
    "            return QuestionAnsweringPipeline.create_sample(**item)\n",
    "        raise ValueError(\"{} argument needs to be of type (SquadExample, dict)\".format(item))\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        # Detect where the actual inputs are\n",
    "        if args is not None and len(args) > 0:\n",
    "            if len(args) == 1:\n",
    "                inputs = args[0]\n",
    "            elif len(args) == 2 and {type(el) for el in args} == {str}:\n",
    "                inputs = [{\"question\": args[0], \"context\": args[1]}]\n",
    "            else:\n",
    "                inputs = list(args)\n",
    "        # Generic compatibility with sklearn and Keras\n",
    "        # Batched data\n",
    "        elif \"X\" in kwargs:\n",
    "            inputs = kwargs[\"X\"]\n",
    "        elif \"data\" in kwargs:\n",
    "            inputs = kwargs[\"data\"]\n",
    "        elif \"question\" in kwargs and \"context\" in kwargs:\n",
    "            if isinstance(kwargs[\"question\"], list) and isinstance(kwargs[\"context\"], str):\n",
    "                inputs = [{\"question\": Q, \"context\": kwargs[\"context\"]} for Q in kwargs[\"question\"]]\n",
    "            elif isinstance(kwargs[\"question\"], list) and isinstance(kwargs[\"context\"], list):\n",
    "                if len(kwargs[\"question\"]) != len(kwargs[\"context\"]):\n",
    "                    raise ValueError(\"Questions and contexts don't have the same lengths\")\n",
    "\n",
    "                inputs = [{\"question\": Q, \"context\": C} for Q, C in zip(kwargs[\"question\"], kwargs[\"context\"])]\n",
    "            elif isinstance(kwargs[\"question\"], str) and isinstance(kwargs[\"context\"], str):\n",
    "                inputs = [{\"question\": kwargs[\"question\"], \"context\": kwargs[\"context\"]}]\n",
    "            else:\n",
    "                raise ValueError(\"Arguments can't be understood\")\n",
    "        else:\n",
    "            raise ValueError(\"Unknown arguments {}\".format(kwargs))\n",
    "\n",
    "        # Normalize inputs\n",
    "        if isinstance(inputs, dict):\n",
    "            inputs = [inputs]\n",
    "        elif isinstance(inputs, Iterable):\n",
    "            # Copy to avoid overriding arguments\n",
    "            inputs = [i for i in inputs]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid arguments {}\".format(inputs))\n",
    "\n",
    "        for i, item in enumerate(inputs):\n",
    "            inputs[i] = self.normalize(item)\n",
    "\n",
    "        return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionAnsweringPipeline:\n",
    "    \"\"\"\n",
    "    Question Answering pipeline using any :obj:`ModelForQuestionAnswering`. See the `question answering examples\n",
    "    <../task_summary.html#question-answering>`__ for more information.\n",
    "\n",
    "    This question answering pipeline can currently be loaded from :func:`~transformers.pipeline` using the following\n",
    "    task identifier: :obj:`\"question-answering\"`.\n",
    "\n",
    "    The models that this pipeline can use are models that have been fine-tuned on a question answering task. See the\n",
    "    up-to-date list of available models on `huggingface.co/models\n",
    "    <https://huggingface.co/models?filter=question-answering>`__.\n",
    "    \"\"\"\n",
    "\n",
    "    default_input_names = \"question,context\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: model,\n",
    "        tokenizer: tokenizer,\n",
    "        modelcard: Optional = None,\n",
    "        framework: Optional[str] = None,\n",
    "        device: int = -1,\n",
    "        task: str = \"\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            modelcard=modelcard,\n",
    "            framework=framework,\n",
    "            device=device,\n",
    "            task=task,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self._args_parser = QuestionAnsweringArgumentHandler()\n",
    "        self.check_model_type(\n",
    "            TF_MODEL_FOR_QUESTION_ANSWERING_MAPPING if self.framework == \"tf\" else MODEL_FOR_QUESTION_ANSWERING_MAPPING\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def create_sample(\n",
    "        question: Union[str, List[str]], context: Union[str, List[str]]\n",
    "    ) -> Union[SquadExample, List[SquadExample]]:\n",
    "        \"\"\"\n",
    "        QuestionAnsweringPipeline leverages the :class:`~transformers.SquadExample` internally. This helper method\n",
    "        encapsulate all the logic for converting question(s) and context(s) to :class:`~transformers.SquadExample`.\n",
    "\n",
    "        We currently support extractive question answering.\n",
    "\n",
    "        Arguments:\n",
    "            question (:obj:`str` or :obj:`List[str]`): The question(s) asked.\n",
    "            context (:obj:`str` or :obj:`List[str]`): The context(s) in which we will look for the answer.\n",
    "\n",
    "        Returns:\n",
    "            One or a list of :class:`~transformers.SquadExample`: The corresponding :class:`~transformers.SquadExample`\n",
    "            grouping question and context.\n",
    "        \"\"\"\n",
    "        if isinstance(question, list):\n",
    "            return [SquadExample(None, q, c, None, None, None) for q, c in zip(question, context)]\n",
    "        else:\n",
    "            return SquadExample(None, question, context, None, None, None)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Answer the question(s) given as inputs by using the context(s).\n",
    "\n",
    "        Args:\n",
    "            args (:class:`~transformers.SquadExample` or a list of :class:`~transformers.SquadExample`):\n",
    "                One or several :class:`~transformers.SquadExample` containing the question and context.\n",
    "            X (:class:`~transformers.SquadExample` or a list of :class:`~transformers.SquadExample`, `optional`):\n",
    "                One or several :class:`~transformers.SquadExample` containing the question and context (will be treated\n",
    "                the same way as if passed as the first positional argument).\n",
    "            data (:class:`~transformers.SquadExample` or a list of :class:`~transformers.SquadExample`, `optional`):\n",
    "                One or several :class:`~transformers.SquadExample` containing the question and context (will be treated\n",
    "                the same way as if passed as the first positional argument).\n",
    "            question (:obj:`str` or :obj:`List[str]`):\n",
    "                One or several question(s) (must be used in conjunction with the :obj:`context` argument).\n",
    "            context (:obj:`str` or :obj:`List[str]`):\n",
    "                One or several context(s) associated with the question(s) (must be used in conjunction with the\n",
    "                :obj:`question` argument).\n",
    "            topk (:obj:`int`, `optional`, defaults to 1):\n",
    "                The number of answers to return (will be chosen by order of likelihood).\n",
    "            doc_stride (:obj:`int`, `optional`, defaults to 128):\n",
    "                If the context is too long to fit with the question for the model, it will be split in several chunks\n",
    "                with some overlap. This argument controls the size of that overlap.\n",
    "            max_answer_len (:obj:`int`, `optional`, defaults to 15):\n",
    "                The maximum length of predicted answers (e.g., only answers with a shorter length are considered).\n",
    "            max_seq_len (:obj:`int`, `optional`, defaults to 384):\n",
    "                The maximum length of the total sentence (context + question) after tokenization. The context will be\n",
    "                split in several chunks (using :obj:`doc_stride`) if needed.\n",
    "            max_question_len (:obj:`int`, `optional`, defaults to 64):\n",
    "                The maximum length of the question after tokenization. It will be truncated if needed.\n",
    "            handle_impossible_answer (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether or not we accept impossible as an answer.\n",
    "\n",
    "        Return:\n",
    "            A :obj:`dict` or a list of :obj:`dict`: Each result comes as a dictionary with the following keys:\n",
    "\n",
    "            - **score** (:obj:`float`) -- The probability associated to the answer.\n",
    "            - **start** (:obj:`int`) -- The start index of the answer (in the tokenized version of the input).\n",
    "            - **end** (:obj:`int`) -- The end index of the answer (in the tokenized version of the input).\n",
    "            - **answer** (:obj:`str`) -- The answer to the question.\n",
    "        \"\"\"\n",
    "        # Set defaults values\n",
    "        kwargs.setdefault(\"padding\", \"longest\")\n",
    "        kwargs.setdefault(\"topk\", 1)\n",
    "        kwargs.setdefault(\"doc_stride\", 128)\n",
    "        kwargs.setdefault(\"max_answer_len\", 15)\n",
    "        kwargs.setdefault(\"max_seq_len\", 384)\n",
    "        kwargs.setdefault(\"max_question_len\", 64)\n",
    "        kwargs.setdefault(\"handle_impossible_answer\", False)\n",
    "\n",
    "        if kwargs[\"topk\"] < 1:\n",
    "            raise ValueError(\"topk parameter should be >= 1 (got {})\".format(kwargs[\"topk\"]))\n",
    "\n",
    "        if kwargs[\"max_answer_len\"] < 1:\n",
    "            raise ValueError(\"max_answer_len parameter should be >= 1 (got {})\".format(kwargs[\"max_answer_len\"]))\n",
    "\n",
    "        # Convert inputs to features\n",
    "        examples = self._args_parser(*args, **kwargs)\n",
    "        if not self.tokenizer.is_fast:\n",
    "            features_list = [\n",
    "                squad_convert_examples_to_features(\n",
    "                    examples=[example],\n",
    "                    tokenizer=self.tokenizer,\n",
    "                    max_seq_length=kwargs[\"max_seq_len\"],\n",
    "                    doc_stride=kwargs[\"doc_stride\"],\n",
    "                    max_query_length=kwargs[\"max_question_len\"],\n",
    "                    padding_strategy=PaddingStrategy.MAX_LENGTH.value,\n",
    "                    is_training=False,\n",
    "                    tqdm_enabled=False,\n",
    "                )\n",
    "                for example in examples\n",
    "            ]\n",
    "        else:\n",
    "            features_list = []\n",
    "            for example in examples:\n",
    "                # Define the side we want to truncate / pad and the text/pair sorting\n",
    "                question_first = bool(self.tokenizer.padding_side == \"right\")\n",
    "\n",
    "                encoded_inputs = self.tokenizer(\n",
    "                    text=example.question_text if question_first else example.context_text,\n",
    "                    text_pair=example.context_text if question_first else example.question_text,\n",
    "                    padding=kwargs[\"padding\"],\n",
    "                    truncation=\"only_second\" if question_first else \"only_first\",\n",
    "                    max_length=kwargs[\"max_seq_len\"],\n",
    "                    stride=kwargs[\"doc_stride\"],\n",
    "                    return_tensors=\"np\",\n",
    "                    return_token_type_ids=True,\n",
    "                    return_overflowing_tokens=True,\n",
    "                    return_offsets_mapping=True,\n",
    "                    return_special_tokens_mask=True,\n",
    "                )\n",
    "\n",
    "                # When the input is too long, it's converted in a batch of inputs with overflowing tokens\n",
    "                # and a stride of overlap between the inputs. If a batch of inputs is given, a special output\n",
    "                # \"overflow_to_sample_mapping\" indicate which member of the encoded batch belong to which original batch sample.\n",
    "                # Here we tokenize examples one-by-one so we don't need to use \"overflow_to_sample_mapping\".\n",
    "                # \"num_span\" is the number of output samples generated from the overflowing tokens.\n",
    "                num_spans = len(encoded_inputs[\"input_ids\"])\n",
    "\n",
    "                # p_mask: mask with 1 for token than cannot be in the answer (0 for token which can be in an answer)\n",
    "                # We put 0 on the tokens from the context and 1 everywhere else (question and special tokens)\n",
    "                p_mask = np.asarray(\n",
    "                    [\n",
    "                        [tok != 1 if question_first else 0 for tok in encoded_inputs.sequence_ids(span_id)]\n",
    "                        for span_id in range(num_spans)\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                # keep the cls_token unmasked (some models use it to indicate unanswerable questions)\n",
    "                if self.tokenizer.cls_token_id:\n",
    "                    cls_index = np.nonzero(encoded_inputs[\"input_ids\"] == self.tokenizer.cls_token_id)\n",
    "                    p_mask[cls_index] = 0\n",
    "\n",
    "                features = []\n",
    "                for span_idx in range(num_spans):\n",
    "                    features.append(\n",
    "                        SquadFeatures(\n",
    "                            input_ids=encoded_inputs[\"input_ids\"][span_idx],\n",
    "                            attention_mask=encoded_inputs[\"attention_mask\"][span_idx],\n",
    "                            token_type_ids=encoded_inputs[\"token_type_ids\"][span_idx],\n",
    "                            p_mask=p_mask[span_idx].tolist(),\n",
    "                            encoding=encoded_inputs[span_idx],\n",
    "                            # We don't use the rest of the values - and actually\n",
    "                            # for Fast tokenizer we could totally avoid using SquadFeatures and SquadExample\n",
    "                            cls_index=None,\n",
    "                            token_to_orig_map={},\n",
    "                            example_index=0,\n",
    "                            unique_id=0,\n",
    "                            paragraph_len=0,\n",
    "                            token_is_max_context=0,\n",
    "                            tokens=[],\n",
    "                            start_position=0,\n",
    "                            end_position=0,\n",
    "                            is_impossible=False,\n",
    "                            qas_id=None,\n",
    "                        )\n",
    "                    )\n",
    "                features_list.append(features)\n",
    "        return features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "object.__init__() takes exactly one argument (the instance to initialize)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-c30c71a19f9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mQuestionAnsweringPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-57-d417d7ad0d4c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, tokenizer, modelcard, framework, device, task, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     ):\n\u001b[0;32m---> 26\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object.__init__() takes exactly one argument (the instance to initialize)"
     ]
    }
   ],
   "source": [
    "QuestionAnsweringPipeline(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = []\n",
    "for example in examples:\n",
    "    # Define the side we want to truncate / pad and the text/pair sorting\n",
    "    question_first = bool(tokenizer.padding_side == \"right\")\n",
    "    encoded_inputs = tokenizer(\n",
    "        text=example[\"question_text\"] if question_first else example[\"context_text\"],\n",
    "        text_pair=example[\"context_text\"] if question_first else example[\"question_text\"],\n",
    "        padding=padding,\n",
    "        truncation=\"only_second\" if question_first else \"only_first\",\n",
    "        max_length=max_seq_len,\n",
    "        stride=doc_stride,\n",
    "        return_tensors=\"np\",\n",
    "        return_token_type_ids=True,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        return_special_tokens_mask=True,\n",
    "        )\n",
    "    \n",
    "    # When the input is too long, it's converted in a batch of inputs with overflowing tokens\n",
    "    # and a stride of overlap between the inputs. If a batch of inputs is given, a special output\n",
    "    # \"overflow_to_sample_mapping\" indicate which member of the encoded batch belong to which original batch sample.\n",
    "    # Here we tokenize examples one-by-one so we don't need to use \"overflow_to_sample_mapping\".\n",
    "    # \"num_span\" is the number of output samples generated from the overflowing tokens.\n",
    "    num_spans = len(encoded_inputs[\"input_ids\"])\n",
    "\n",
    "    # p_mask: mask with 1 for token than cannot be in the answer (0 for token which can be in an answer)\n",
    "    # We put 0 on the tokens from the context and 1 everywhere else (question and special tokens)\n",
    "    p_mask = np.asarray(\n",
    "        [\n",
    "            [tok != 1 if question_first else 0 for tok in encoded_inputs.sequence_ids(span_id)]\n",
    "            for span_id in range(num_spans)\n",
    "        ]\n",
    "    )\n",
    "    # keep the cls_token unmasked (some models use it to indicate unanswerable questions)\n",
    "    if tokenizer.cls_token_id:\n",
    "        cls_index = np.nonzero(encoded_inputs[\"input_ids\"] == self.tokenizer.cls_token_id)\n",
    "        p_mask[cls_index] = 0\n",
    "\n",
    "    features = []\n",
    "    for span_idx in range(num_spans):\n",
    "        features.append({\n",
    "                \"input_ids\":encoded_inputs[\"input_ids\"][span_idx],\n",
    "                \"attention_mask\":encoded_inputs[\"attention_mask\"][span_idx],\n",
    "                \"token_type_ids\":encoded_inputs[\"token_type_ids\"][span_idx],\n",
    "                \"p_mask\":p_mask[span_idx].tolist(),\n",
    "                \"encoding\":encoded_inputs[span_idx]}\n",
    "                )\n",
    "    features_list.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': array([    0,  8569,    16,     5,  1252,  8348,     7,   213,   116,\n",
       "            2,     2,  1121,    63,   419,   107,     6,     5,    92,\n",
       "         8825,  1312,  1447,     7,   972,  6856,     8,   903,  2113,\n",
       "        31274,  1092,   742,   870,  5241,     6,   171, 10087,  1739,\n",
       "         1252,    58,  8348,     5,   203,  2514,  8033, 33666,   824,\n",
       "           11,   764,  2659,    81,     5,   764,  3071,  9127,   824,\n",
       "          528,     7,     5,  5442,    18,  1804,   980,     4,    83,\n",
       "         5250,  2450,     7,  2879,    41,  2919,  1241,    10,  2303,\n",
       "          629,  1447,     7,  1338,     5,  1552,    80,    12, 10224,\n",
       "         1647,     7,  1323,     4,    96,   502,  4013,     6,  2711,\n",
       "          764,  3071,  1490,     5,   391,  1631,     6,    10,    68,\n",
       "          401,     4,  4718,   153,     6,  2440,     8,  1104, 10178,\n",
       "            6,  1271,  1812,     6,   151,  3925,  1730,    36,   406,\n",
       "            6,  4017,   475,   176,    43,     9,  8483,   980,     2]),\n",
       " 'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'token_type_ids': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'p_mask': [True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True],\n",
       " 'encoding': Encoding(num_tokens=126, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])}"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "features_list[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'encoding': Encoding(num_tokens=126, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            SquadFeatures(\n",
    "                input_ids=encoded_inputs[\"input_ids\"][span_idx],\n",
    "                attention_mask=encoded_inputs[\"attention_mask\"][span_idx],\n",
    "                token_type_ids=encoded_inputs[\"token_type_ids\"][span_idx],\n",
    "                p_mask=p_mask[span_idx].tolist(),\n",
    "                encoding=encoded_inputs[span_idx],\n",
    "                # We don't use the rest of the values - and actually\n",
    "                # for Fast tokenizer we could totally avoid using SquadFeatures and SquadExample\n",
    "                cls_index=None,\n",
    "                token_to_orig_map={},\n",
    "                example_index=0,\n",
    "                unique_id=0,\n",
    "                paragraph_len=0,\n",
    "                token_is_max_context=0,\n",
    "                tokens=[],\n",
    "                start_position=0,\n",
    "                end_position=0,\n",
    "                is_impossible=False,\n",
    "                qas_id=None,\n",
    "                )\n",
    "            )\n",
    "    features_list.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.data.processors.squad import squad_convert_example_to_features\n",
    "\n",
    "examples = [{'context_text': \"In its early years, the new convention center failed to meet attendance and revenue expectations.[12] By 2002, many Silicon Valley businesses were choosing the much larger Moscone Center in San Francisco over the San Jose Convention Center due to the latter's limited space. A ballot measure to finance an expansion via a hotel tax failed to reach the required two-thirds majority to pass. In June 2005, Team San Jose built the South Hall, a $6.77 million, blue and white tent, adding 80,000 square feet (7,400 m2) of exhibit space\",\n",
    "'question_text': \"where is the businesses choosing to go?\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.tokenization_utils_base import PaddingStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "squad_convert_example_to_features() got an unexpected keyword argument 'examples'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-bc313f13a982>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msquad_convert_example_to_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m384\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_stride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_query_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: squad_convert_example_to_features() got an unexpected keyword argument 'examples'"
     ]
    }
   ],
   "source": [
    "squad_convert_example_to_features(examples=[examples], tokenizer=tokenizer, max_seq_length=384, doc_stride=128, max_query_length=64, is_training=False, return_dataset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "squad_convert_example_to_features() got an unexpected keyword argument 'examples'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-715b8bfc9e64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m features_list=squad_convert_example_to_features(examples=[examples],\n\u001b[0m\u001b[1;32m      2\u001b[0m         \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m384\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mdoc_stride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mmax_query_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: squad_convert_example_to_features() got an unexpected keyword argument 'examples'"
     ]
    }
   ],
   "source": [
    "features_list=squad_convert_example_to_features(examples=[examples],\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=384,\n",
    "        doc_stride=128,\n",
    "        max_query_length=64,\n",
    "        padding_strategy=PaddingStrategy.MAX_LENGTH.value,\n",
    "        is_training=False,\n",
    "        tqdm_enabled=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"deepset/roberta-base-squad2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "from transformers import pipeline\n",
    "nlp=pipeline(task=\"question-answering\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'ArgumentHandler' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-06fafd4aa376>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mArgumentHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mABS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ArgumentHandler' is not defined"
     ]
    }
   ],
   "source": [
    "ArgumentHandler(ABS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-fef7eef61126>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransformers_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"context\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"context_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"question\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformers_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m384\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_stride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "transformers_query = {\"context\": examples[\"context_text\"], \"question\": examples[\"question_text\"]}\n",
    "predictions = nlp(transformers_query, topk=1, max_seq_len=384, doc_stride=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = InferenceSession(onnx_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NodeArg(name='input_ids', type='tensor(int64)', shape=['batch', 'sequence'])\nNodeArg(name='attention_mask', type='tensor(int64)', shape=['batch', 'sequence'])\n"
     ]
    }
   ],
   "source": [
    "for input_meta in session.get_inputs():\n",
    "    print(input_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "context=\"In its early years, the new convention center failed to meet attendance and revenue expectations.[12] By 2002, many Silicon Valley businesses were choosing the much larger Moscone Center in San Francisco over the San Jose Convention Center due to the latter's limited space. A ballot measure to finance an expansion via a hotel tax failed to reach the required two-thirds majority to pass. In June 2005, Team San Jose built the South Hall, a $6.77 million, blue and white tent, adding 80,000 square feet (7,400 m2) of exhibit space\"\n",
    "tokens_context = tokenizer(context, return_attention_mask=True, return_tensors=\"np\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"where is the businesses choosing to go?\"\n",
    "tokens_question = tokenizer(question, return_attention_mask=True, return_tensors=\"np\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': array([[   0, 8569,   16,    5, 1252, 8348,    7,  213,  116,    2]]),\n",
       " 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "tokens_question.__dict__[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': array([[    0,  1121,    63,   419,   107,     6,     5,    92,  8825,\n",
       "          1312,  1447,     7,   972,  6856,     8,   903,  2113, 31274,\n",
       "          1092,   742,   870,  5241,     6,   171, 10087,  1739,  1252,\n",
       "            58,  8348,     5,   203,  2514,  8033, 33666,   824,    11,\n",
       "           764,  2659,    81,     5,   764,  3071,  9127,   824,   528,\n",
       "             7,     5,  5442,    18,  1804,   980,     4,    83,  5250,\n",
       "          2450,     7,  2879,    41,  2919,  1241,    10,  2303,   629,\n",
       "          1447,     7,  1338,     5,  1552,    80,    12, 10224,  1647,\n",
       "             7,  1323,     4,    96,   502,  4013,     6,  2711,   764,\n",
       "          3071,  1490,     5,   391,  1631,     6,    10,    68,   401,\n",
       "             4,  4718,   153,     6,  2440,     8,  1104, 10178,     6,\n",
       "          1271,  1812,     6,   151,  3925,  1730,    36,   406,     6,\n",
       "          4017,   475,   176,    43,     9,  8483,   980,     2]]),\n",
       " 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "tokens_context.__dict__[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}