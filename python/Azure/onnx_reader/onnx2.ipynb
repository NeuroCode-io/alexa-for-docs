{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.1 64-bit ('.venv')",
   "metadata": {
    "interpreter": {
     "hash": "4037c546effa33586e051666d7d74862170a63a04006db20983879415d1a70a6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime import InferenceSession\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from pathlib import Path\n",
    "from transformers.convert_graph_to_onnx import convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_name=\"./onnx_model/roberta-base-squad2.onnx\"\n",
    "model_name=\"deepset/roberta-base-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_for_onnx(model_path, onnx_model_name, tokenizer):\n",
    "    if Path(onnx_model_name).exists():\n",
    "        print(\"ONNX input exists\")\n",
    "        return\n",
    "    convert(\n",
    "        framework=\"pt\",\n",
    "        model=model_path,\n",
    "        tokenizer=tokenizer,\n",
    "        output=Path(onnx_model_name),\n",
    "        pipeline_name=\"question-answering\",\n",
    "        opset=12\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ONNX input exists\n"
     ]
    }
   ],
   "source": [
    "convert_for_onnx(model_path=model, onnx_model_name=onnx_model_name, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need example.context_text and example.question_text\n",
    "example_dict={\"context\": \"In its early years, the new convention center failed to meet attendance and revenue expectations.[12] By 2002, many Silicon Valley businesses were choosing the much larger Moscone Center in San Francisco over the San Jose Convention Center due to the latter's limited space. A ballot measure to finance an expansion via a hotel tax failed to reach the required two-thirds majority to pass. In June 2005, Team San Jose built the South Hall, a $6.77 million, blue and white tent, adding 80,000 square feet (7,400 m2) of exhibit space\", \"question\": \"where is the businesses choosing to go?\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding=\"longest\"\n",
    "max_seq_len=384\n",
    "doc_stride=128\n",
    "max_question_len=64\n",
    "max_answer_len=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.data.processors import squad_convert_examples_to_features\n",
    "from transformers.pipelines import QuestionAnsweringArgumentHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "_arg_parser=QuestionAnsweringArgumentHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples= _arg_parser(example_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.data.processors import SquadFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = []\n",
    "for example in examples:\n",
    "    # Define the side we want to truncate / pad and the text/pair sorting\n",
    "    question_first = bool(tokenizer.padding_side == \"right\")\n",
    "    encoded_inputs = tokenizer(\n",
    "        text=example.question_text if question_first else example.context_text,\n",
    "        text_pair=example.context_text if question_first else example.question_text,\n",
    "        padding=padding,\n",
    "        truncation=\"only_second\" if question_first else \"only_first\",\n",
    "        max_length=max_seq_len,\n",
    "        stride=doc_stride,\n",
    "        return_tensors=\"np\",\n",
    "        return_token_type_ids=True,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "    \n",
    "    num_spans = len(encoded_inputs[\"input_ids\"])\n",
    "    p_mask = np.asarray(\n",
    "        [\n",
    "            [tok != 1 if question_first else 0 for tok in encoded_inputs.sequence_ids(span_id)]\n",
    "            for span_id in range(num_spans)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # keep the cls_token unmasked (some models use it to indicate unanswerable questions)\n",
    "    if tokenizer.cls_token_id:\n",
    "        cls_index = np.nonzero(encoded_inputs[\"input_ids\"] == tokenizer.cls_token_id)\n",
    "        p_mask[cls_index] = 0\n",
    "\n",
    "    features = []\n",
    "    for span_idx in range(num_spans):\n",
    "        features.append(\n",
    "            SquadFeatures(\n",
    "                input_ids=encoded_inputs[\"input_ids\"][span_idx],\n",
    "                attention_mask=encoded_inputs[\"attention_mask\"][span_idx],\n",
    "                token_type_ids=encoded_inputs[\"token_type_ids\"][span_idx],\n",
    "                p_mask=p_mask[span_idx].tolist(),\n",
    "                encoding=encoded_inputs[span_idx],\n",
    "                # We don't use the rest of the values - and actually\n",
    "                # for Fast tokenizer we could totally avoid using SquadFeatures and SquadExample\n",
    "                cls_index=None,\n",
    "                token_to_orig_map={},\n",
    "                example_index=0,\n",
    "                unique_id=0,\n",
    "                paragraph_len=0,\n",
    "                token_is_max_context=0,\n",
    "                tokens=[],\n",
    "                start_position=0,\n",
    "                end_position=0,\n",
    "                is_impossible=False,\n",
    "                qas_id=None,\n",
    "                )\n",
    "            )\n",
    "        features_list.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[<transformers.data.processors.squad.SquadFeatures at 0x7f6933b74880>]]"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for features, example in zip(features_list, examples):\n",
    "    model_input_names = tokenizer.model_input_names + [\"input_ids\"]\n",
    "    fw_args = {k: [feature.__dict__[k] for feature in features] for k in model_input_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'attention_mask': [array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])],\n",
       " 'input_ids': [array([    0,  8569,    16,     5,  1252,  8348,     7,   213,   116,\n",
       "             2,     2,  1121,    63,   419,   107,     6,     5,    92,\n",
       "          8825,  1312,  1447,     7,   972,  6856,     8,   903,  2113,\n",
       "         31274,  1092,   742,   870,  5241,     6,   171, 10087,  1739,\n",
       "          1252,    58,  8348,     5,   203,  2514,  8033, 33666,   824,\n",
       "            11,   764,  2659,    81,     5,   764,  3071,  9127,   824,\n",
       "           528,     7,     5,  5442,    18,  1804,   980,     4,    83,\n",
       "          5250,  2450,     7,  2879,    41,  2919,  1241,    10,  2303,\n",
       "           629,  1447,     7,  1338,     5,  1552,    80,    12, 10224,\n",
       "          1647,     7,  1323,     4,    96,   502,  4013,     6,  2711,\n",
       "           764,  3071,  1490,     5,   391,  1631,     6,    10,    68,\n",
       "           401,     4,  4718,   153,     6,  2440,     8,  1104, 10178,\n",
       "             6,  1271,  1812,     6,   151,  3925,  1730,    36,   406,\n",
       "             6,  4017,   475,   176,    43,     9,  8483,   980,     2])]}"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "fw_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = InferenceSession(onnx_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NodeArg(name='input_ids', type='tensor(int64)', shape=['batch', 'sequence'])\nNodeArg(name='attention_mask', type='tensor(int64)', shape=['batch', 'sequence'])\n"
     ]
    }
   ],
   "source": [
    "for inp in session.get_inputs():\n",
    "    print(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(start: np.ndarray, end: np.ndarray, topk: int, max_answer_len: int) -> Tuple:\n",
    "    \"\"\"\n",
    "    Take the output of any :obj:`ModelForQuestionAnswering` and will generate probabilities for each span to be the\n",
    "    actual answer.\n",
    "    In addition, it filters out some unwanted/impossible cases like answer len being greater than max_answer_len or\n",
    "    answer end position being before the starting position. The method supports output the k-best answer through\n",
    "    the topk argument.\n",
    "    Args:\n",
    "        start (:obj:`np.ndarray`): Individual start probabilities for each token.\n",
    "        end (:obj:`np.ndarray`): Individual end probabilities for each token.\n",
    "        topk (:obj:`int`): Indicates how many possible answer span(s) to extract from the model output.\n",
    "        max_answer_len (:obj:`int`): Maximum size of the answer to extract from the model's output.\n",
    "    \"\"\"\n",
    "    # Ensure we have batch axis\n",
    "    if start.ndim == 1:\n",
    "        start = start[None]\n",
    "\n",
    "    if end.ndim == 1:\n",
    "        end = end[None]\n",
    "\n",
    "    # Compute the score of each tuple(start, end) to be the real answer\n",
    "    outer = np.matmul(np.expand_dims(start, -1), np.expand_dims(end, 1))\n",
    "\n",
    "    # Remove candidate with end < start and end - start > max_answer_len\n",
    "    candidates = np.tril(np.triu(outer), max_answer_len - 1)\n",
    "\n",
    "    #  Inspired by Chen & al. (https://github.com/facebookresearch/DrQA)\n",
    "    scores_flat = candidates.flatten()\n",
    "    if topk == 1:\n",
    "        idx_sort = [np.argmax(scores_flat)]\n",
    "    elif len(scores_flat) < topk:\n",
    "        idx_sort = np.argsort(-scores_flat)\n",
    "    else:\n",
    "        idx = np.argpartition(-scores_flat, topk)[0:topk]\n",
    "        idx_sort = idx[np.argsort(-scores_flat[idx])]\n",
    "\n",
    "    start, end = np.unravel_index(idx_sort, candidates.shape)[1:]\n",
    "    return start, end, candidates[0, start, end]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_answers = []\n",
    "for features, example in zip(features_list, examples):\n",
    "    model_input_names = tokenizer.model_input_names + [\"input_ids\"]\n",
    "    fw_args = {k: [feature.__dict__[k] for feature in features] for k in model_input_names}\n",
    "    session = InferenceSession(onnx_model_name)\n",
    "    output = session.run(None, fw_args)\n",
    "    start=output[0][0]\n",
    "    end=output[1][0]\n",
    "\n",
    "    min_null_score = 1000000  # large and positive\n",
    "    answers = []\n",
    "    for (feature, start_, end_) in zip(features, start, end):\n",
    "        # Ensure padded tokens & question tokens cannot belong to the set of candidate answers.\n",
    "        undesired_tokens = np.abs(np.array(feature.p_mask) - 1) & feature.attention_mask\n",
    "    \n",
    "        # Generate mask\n",
    "        undesired_tokens_mask = undesired_tokens == 0.0\n",
    "\n",
    "        # Make sure non-context indexes in the tensor cannot contribute to the softmax\n",
    "        start_ = np.where(undesired_tokens_mask, -10000.0, start_)\n",
    "        end_ = np.where(undesired_tokens_mask, -10000.0, end_)\n",
    "\n",
    "        # Normalize logits and spans to retrieve the answer\n",
    "        start_ = np.exp(start_ - np.log(np.sum(np.exp(start_), axis=-1, keepdims=True)))\n",
    "        end_ = np.exp(end_ - np.log(np.sum(np.exp(end_), axis=-1, keepdims=True)))\n",
    "\n",
    "        # Mask CLS\n",
    "        start_[0] = end_[0] = 0.0\n",
    "        starts, ends, scores = decode(start=start, end=end,topk=1, max_answer_len=max_answer_len)\n",
    "        if not tokenizer.is_fast:\n",
    "            char_to_word = np.array(example.char_to_word_offset)\n",
    "            answers += [\n",
    "                {\n",
    "                    \"score\": score.item(),\n",
    "                    \"start\": np.where(char_to_word == feature.token_to_orig_map[s])[0][0].item(),\n",
    "                    \"end\": np.where(char_to_word == feature.token_to_orig_map[e])[0][-1].item(),\n",
    "                    \"answer\": \" \".join(\n",
    "                        example.doc_tokens[feature.token_to_orig_map[s] : feature.token_to_orig_map[e] + 1]\n",
    "                        ),\n",
    "                }\n",
    "                for s, e, score in zip(starts, ends, scores)\n",
    "            ]\n",
    "        else:\n",
    "            question_first = bool(tokenizer.padding_side == \"right\")\n",
    "            enc = feature.encoding\n",
    "            # Sometimes the max probability token is in the middle of a word so:\n",
    "            # - we start by finding the right word containing the token with `token_to_word`\n",
    "            # - then we convert this word in a character span with `word_to_chars`\n",
    "            answers += [\n",
    "                {\n",
    "                    \"score\": score.item(),\n",
    "                    \"start\": enc.word_to_chars(\n",
    "                    enc.token_to_word(s), sequence_index=1 if question_first else 0 )[0],\n",
    "                    \"end\": enc.word_to_chars(enc.token_to_word(e), sequence_index=1 if question_first else 0)[1],\n",
    "                    \"answer\": example.context_text[\n",
    "                        enc.word_to_chars(enc.token_to_word(s), sequence_index=1 if question_first else 0)[0] : enc.word_to_chars(enc.token_to_word(e), sequence_index=1 if question_first else 0)[1]],\n",
    "                }\n",
    "                for s, e, score in zip(starts, ends, scores)\n",
    "            ]\n",
    "        answers = sorted(answers, key=lambda x: x[\"score\"], reverse=True)[: 1]\n",
    "        all_answers += answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'score': 96.2432861328125, 'start': 438, 'end': 443, 'answer': ', a $'}]"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "all_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "device=torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_answers = []\n",
    "for features, example in zip(features_list, examples):\n",
    "    model_input_names = tokenizer.model_input_names + [\"input_ids\"]\n",
    "    fw_args = {k: [feature.__dict__[k] for feature in features] for k in model_input_names}\n",
    "    with torch.no_grad():\n",
    "        fw_args = {k: torch.tensor(v, device=device) for (k, v) in fw_args.items()}\n",
    "        # On Windows, the default int type in numpy is np.int32 so we get some non-long tensors.\n",
    "        fw_args = {k: v.long() if v.dtype == torch.int32 else v for (k, v) in fw_args.items()}\n",
    "        start, end = model(**fw_args)[:2]\n",
    "        start, end = start.cpu().numpy(), end.cpu().numpy()\n",
    "\n",
    "    min_null_score = 1000000  # large and positive\n",
    "    answers = []\n",
    "    for (feature, start_, end_) in zip(features, start, end):\n",
    "        # Ensure padded tokens & question tokens cannot belong to the set of candidate answers.\n",
    "        undesired_tokens = np.abs(np.array(feature.p_mask) - 1) & feature.attention_mask\n",
    "    \n",
    "        # Generate mask\n",
    "        undesired_tokens_mask = undesired_tokens == 0.0\n",
    "\n",
    "        # Make sure non-context indexes in the tensor cannot contribute to the softmax\n",
    "        start_ = np.where(undesired_tokens_mask, -10000.0, start_)\n",
    "        end_ = np.where(undesired_tokens_mask, -10000.0, end_)\n",
    "\n",
    "        # Normalize logits and spans to retrieve the answer\n",
    "        start_ = np.exp(start_ - np.log(np.sum(np.exp(start_), axis=-1, keepdims=True)))\n",
    "        end_ = np.exp(end_ - np.log(np.sum(np.exp(end_), axis=-1, keepdims=True)))\n",
    "\n",
    "        # Mask CLS\n",
    "        start_[0] = end_[0] = 0.0\n",
    "        starts, ends, scores = decode(start=start, end=end,topk=1, max_answer_len=max_answer_len)\n",
    "        if not tokenizer.is_fast:\n",
    "            char_to_word = np.array(example.char_to_word_offset)\n",
    "            answers += [\n",
    "                {\n",
    "                    \"score\": score.item(),\n",
    "                    \"start\": np.where(char_to_word == feature.token_to_orig_map[s])[0][0].item(),\n",
    "                    \"end\": np.where(char_to_word == feature.token_to_orig_map[e])[0][-1].item(),\n",
    "                    \"answer\": \" \".join(\n",
    "                        example.doc_tokens[feature.token_to_orig_map[s] : feature.token_to_orig_map[e] + 1]\n",
    "                        ),\n",
    "                }\n",
    "                for s, e, score in zip(starts, ends, scores)\n",
    "            ]\n",
    "        else:\n",
    "            question_first = bool(tokenizer.padding_side == \"right\")\n",
    "            enc = feature.encoding\n",
    "            # Sometimes the max probability token is in the middle of a word so:\n",
    "            # - we start by finding the right word containing the token with `token_to_word`\n",
    "            # - then we convert this word in a character span with `word_to_chars`\n",
    "            answers += [\n",
    "                {\n",
    "                    \"score\": score.item(),\n",
    "                    \"start\": enc.word_to_chars(\n",
    "                    enc.token_to_word(s), sequence_index=1 if question_first else 0 )[0],\n",
    "                    \"end\": enc.word_to_chars(enc.token_to_word(e), sequence_index=1 if question_first else 0)[1],\n",
    "                    \"answer\": example.context_text[\n",
    "                        enc.word_to_chars(enc.token_to_word(s), sequence_index=1 if question_first else 0)[0] : enc.word_to_chars(enc.token_to_word(e), sequence_index=1 if question_first else 0)[1]],\n",
    "                }\n",
    "                for s, e, score in zip(starts, ends, scores)\n",
    "            ]\n",
    "        answers = sorted(answers, key=lambda x: x[\"score\"], reverse=True)[: 1]\n",
    "        all_answers += answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'score': 96.2432861328125, 'start': 438, 'end': 443, 'answer': ', a $'}]"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "all_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}