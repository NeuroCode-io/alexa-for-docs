{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.1 64-bit ('.venv')",
   "metadata": {
    "interpreter": {
     "hash": "4037c546effa33586e051666d7d74862170a63a04006db20983879415d1a70a6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime import InferenceSession\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from pathlib import Path\n",
    "from transformers.convert_graph_to_onnx import convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_name=\"./onnx_model/roberta-base-squad2.onnx\"\n",
    "model_name=\"deepset/roberta-base-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_for_onnx(model_path, onnx_model_name, tokenizer):\n",
    "    if Path(onnx_model_name).exists():\n",
    "        print(\"ONNX input exists\")\n",
    "        return\n",
    "    convert(\n",
    "        framework=\"pt\",\n",
    "        model=model_path,\n",
    "        tokenizer=tokenizer,\n",
    "        output=Path(onnx_model_name),\n",
    "        pipeline_name=\"question-answering\",\n",
    "        opset=12\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ONNX input exists\n"
     ]
    }
   ],
   "source": [
    "convert_for_onnx(model_path=model, onnx_model_name=onnx_model_name, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need example.context_text and example.question_text\n",
    "example_dict={\"context\": \"Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune a model on a SQuAD task, you may leverage the `run_squad.py`.\", \"question\": \"What is extractive question answering?\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnx_utils import get_examples, get_features, predict_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples=get_examples(example_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list=get_features(examples=examples, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'score': 0.21330079436302185,\n",
       " 'start': 71,\n",
       " 'end': 94,\n",
       " 'answer': 'a text given a question'}"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "predict_qa(onnx_model_name, tokenizer, features_list, examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_onnx=predict_qa(model_type=\"onnx\", features_list=features_list, examples=examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "session=InferenceSession(onnx_model_name)\n",
    "inputs = tokenizer(example_dict[\"question\"], example_dict[\"context\"], add_special_tokens=True, return_tensors=\"np\")\n",
    "input_ids = inputs[\"input_ids\"][0]\n",
    "text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "output = session.run(None, inputs.__dict__[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Score: [91.88429  91.55705  91.011696]\n"
     ]
    }
   ],
   "source": [
    "start = output[0]\n",
    "end = output[1]\n",
    "outer = np.matmul(np.expand_dims(start, -1), np.expand_dims(end, 1))\n",
    "candidates = np.tril(np.triu(outer), 99)\n",
    "\n",
    "scores_flat = candidates.flatten()\n",
    "topk=3\n",
    "if topk == 1:\n",
    "    idx_sort = [np.argmax(scores_flat)]\n",
    "elif len(scores_flat) < topk:\n",
    "    idx_sort = np.argsort(-scores_flat)\n",
    "else:\n",
    "    idx = np.argpartition(-scores_flat, topk)[0:topk]\n",
    "    idx_sort = idx[np.argsort(-scores_flat[idx])]\n",
    "s, e = np.unravel_index(idx_sort, candidates.shape)[1:]\n",
    "print(f\"Score: {candidates[0, s, e]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([43, 43, 43]), array([65, 52, 64]))"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "s, e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ", which is entirely based on that task. If you would like to fine-tune a model on a\n, which is entirely based on that task.\n, which is entirely based on that task. If you would like to fine-tune a model on\n"
     ]
    }
   ],
   "source": [
    "for start, end in zip(s, e):\n",
    "    answer=tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[start:end]))\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Question: How many pretrained models are available in ðŸ¤— Transformers?\n",
      "Answer:  over 32+\n",
      "Question: What does ðŸ¤— Transformers provide?\n",
      "Answer:  general-purpose\n",
      "architectures\n",
      "Question: ðŸ¤— Transformers provides interoperability between which frameworks?\n",
      "Answer: <s>ðŸ¤— Transformers provides interoperability between which frameworks?</s></s> \n",
      "ðŸ¤— Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
      "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and Natural\n",
      "Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
      "TensorFlow 2.0 and PyTorch\n"
     ]
    }
   ],
   "source": [
    "text = r\"\"\" \n",
    "ðŸ¤— Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
    "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and Natural\n",
    "Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
    "TensorFlow 2.0 and PyTorch.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"How many pretrained models are available in ðŸ¤— Transformers?\",\n",
    "    \"What does ðŸ¤— Transformers provide?\",\n",
    "    \"ðŸ¤— Transformers provides interoperability between which frameworks?\",\n",
    "]\n",
    "\n",
    "session=InferenceSession(onnx_model_name)\n",
    "for question in questions:\n",
    "    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=\"np\")\n",
    "    input_ids = inputs[\"input_ids\"][0]\n",
    "    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    outputs = session.run(None, inputs.__dict__[\"data\"])\n",
    "    answer_start_scores = outputs[0]\n",
    "    answer_end_scores = outputs[1]\n",
    "    answer_start = np.argmax(answer_start_scores)  # Get the most likely beginning of answer with the argmax of the score\n",
    "    answer_end = np.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}