{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.1 64-bit ('.venv': venv)",
   "metadata": {
    "interpreter": {
     "hash": "5bdca235408bc63495fd8719d073449338626e85bb6ce08bacdb45b63101d775"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.preprocessor.utils import convert_files_to_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "12/09/2020 10:56:21 - INFO - haystack.preprocessor.utils -   Converting data/9781839217579-THE_DEEP_LEARNING_WITH_KERAS_WORKSHOP_SECOND_EDITION.pdf\n"
     ]
    }
   ],
   "source": [
    "dicts=convert_files_to_dicts(\"./data/\", split_paragraphs=True) # no cleaning function applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(list, 1418, dict, dict_keys(['text', 'meta']))"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "type(dicts), len(dicts), type(dicts[0]), dicts[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'text': '\\x0cThe Deep Learning\\nwith Keras Workshop\\nSecond Edition', 'meta': {'name': '9781839217579-THE_DEEP_LEARNING_WITH_KERAS_WORKSHOP_SECOND_EDITION.pdf'}}\n"
     ]
    }
   ],
   "source": [
    "print(dicts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.document_store.faiss import FAISSDocumentStore\n",
    "\n",
    "document_store = FAISSDocumentStore()\n",
    "document_store.delete_all_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store.write_documents(dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "12/09/2020 10:56:39 - WARNING - farm.modeling.language_model -   Could not automatically detect from language model name what language it is. \n",
      "\t We guess it's an *ENGLISH* model ... \n",
      "\t If not: Init the language model by supplying the 'language' param.\n",
      "12/09/2020 10:56:46 - WARNING - farm.modeling.language_model -   Could not automatically detect from language model name what language it is. \n",
      "\t We guess it's an *ENGLISH* model ... \n",
      "\t If not: Init the language model by supplying the 'language' param.\n"
     ]
    }
   ],
   "source": [
    "from haystack.retriever.dense import DensePassageRetriever\n",
    "retriever = DensePassageRetriever(document_store=document_store,\n",
    "                                  query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n",
    "                                  passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\n",
    "                                  max_seq_len_query=64,\n",
    "                                  max_seq_len_passage=256,\n",
    "                                  batch_size=16,\n",
    "                                  use_gpu=False,\n",
    "                                  embed_title=True,\n",
    "                                  use_fast_tokenizers=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "12/09/2020 10:56:58 - INFO - haystack.document_store.faiss -   Updating embeddings for 1418 docs...\n",
      "Inferencing Samples: 100%|██████████| 89/89 [11:22<00:00,  7.67s/ Batches]\n",
      "12/09/2020 11:08:23 - INFO - haystack.document_store.faiss -   Indexing embeddings and updating vectors_ids...\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.54it/s]\n"
     ]
    }
   ],
   "source": [
    "document_store.update_embeddings(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "12/09/2020 11:08:31 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n",
      "12/09/2020 11:08:31 - INFO - farm.infer -   Could not find `deepset/roberta-base-squad2` locally. Try to download from model hub ...\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at deepset/roberta-base-squad2 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "12/09/2020 11:08:36 - WARNING - farm.modeling.language_model -   Could not automatically detect from language model name what language it is. \n",
      "\t We guess it's an *ENGLISH* model ... \n",
      "\t If not: Init the language model by supplying the 'language' param.\n",
      "12/09/2020 11:08:45 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n",
      "12/09/2020 11:08:45 - INFO - farm.infer -   Got ya 3 parallel workers to do inference ...\n",
      "12/09/2020 11:08:45 - INFO - farm.infer -    0    0    0 \n",
      "12/09/2020 11:08:45 - INFO - farm.infer -   /w\\  /w\\  /w\\\n",
      "12/09/2020 11:08:45 - INFO - farm.infer -   /'\\  / \\  /'\\\n",
      "12/09/2020 11:08:45 - INFO - farm.infer -       \n",
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-2:\n",
      "Process ForkPoolWorker-1:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.8/multiprocessing/queues.py\", line 356, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/local/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.8/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/local/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/local/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/local/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "from haystack.reader.farm import FARMReader\n",
    "farm_reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Finder\n",
    "finder = Finder(farm_reader, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.99 Batches/s]\n",
      "12/09/2020 11:08:51 - INFO - haystack.finder -   Got 10 candidates from retriever\n",
      "12/09/2020 11:08:51 - INFO - haystack.finder -   Reader is looking for detailed answer in 3458 chars ...\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.05 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.47 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.19 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.20 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.09 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.05 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.00 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.08 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.06 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.83 Batches/s]\n"
     ]
    }
   ],
   "source": [
    "prediction = finder.get_answers(question=\"What is RNN?\", top_k_retriever=10, top_k_reader=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.utils import print_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[   {   'answer': 'the hidden layer not only gives the\\n'\n                  'output, but it also feeds back the information of the '\n                  'output into itself',\n        'context': 'y of the RNN is that the hidden layer not only gives the\\n'\n                   'output, but it also feeds back the information of the '\n                   'output into itself. Before taking a\\n'\n                   'dee',\n        'score': 12.397671699523926},\n    {   'answer': 'Recurrent Neural Networks',\n        'context': 'Recurrent Neural Networks (RNNs)\\n'\n                   'RNNs are a class of neural networks that are built on the '\n                   'concept of sequential\\n'\n                   'memory. Unlike traditional neural net',\n        'score': 11.025992393493652},\n    {   'answer': 'Long Short-Term Memory',\n        'context': 'Long Short-Term Memory (LSTM)\\n'\n                   'LSTMs are RNNs whose main objective is to overcome the '\n                   'shortcomings of the vanishing\\n'\n                   'gradient and exploding gradient pro',\n        'score': 4.55922269821167}]\n"
     ]
    }
   ],
   "source": [
    "print_answers(prediction, details=\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.89 Batches/s]\n",
      "12/09/2020 11:09:04 - INFO - haystack.finder -   Got 10 candidates from retriever\n",
      "12/09/2020 11:09:04 - INFO - haystack.finder -   Reader is looking for detailed answer in 5180 chars ...\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.33s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.45 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.52 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.45s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.07 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.22 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.89 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.11 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.03 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.97 Batches/s]\n"
     ]
    }
   ],
   "source": [
    "prediction = finder.get_answers(question=\"What is a layer?\", top_k_retriever=10, top_k_reader=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[   {   'answer': 'Dense layer',\n        'context': 'ras. For now, we will use only the simplest type of\\n'\n                   'layer, called the Dense layer. A Dense layer is equivalent '\n                   'to the fully connected\\n'\n                   'layers that we h',\n        'score': 10.62425708770752},\n    {   'answer': 'a\\ncomposition of nodes',\n        'context': 'ers is part of the Keras core API. A layer can be thought '\n                   'of as a\\n'\n                   'composition of nodes, and at each node, a set of '\n                   'computations happen. In Keras, all ',\n        'score': 10.560615539550781},\n    {   'answer': 'convolutional',\n        'context': 'd: it has a four-dimensional input shape (None, 224, 224,\\n'\n                   '3) and it has three convolutional layers.\\n'\n                   'The last four layers of the output are as follows:',\n        'score': 9.095014572143555}]\n"
     ]
    }
   ],
   "source": [
    "print_answers(prediction, details=\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.34 Batches/s]\n",
      "12/09/2020 11:09:17 - INFO - haystack.finder -   Got 10 candidates from retriever\n",
      "12/09/2020 11:09:17 - INFO - haystack.finder -   Reader is looking for detailed answer in 3292 chars ...\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.06s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.06 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.82 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.00 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.92 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.63 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.91 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.13 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.33s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.14 Batches/s]\n"
     ]
    }
   ],
   "source": [
    "prediction = finder.get_answers(question=\"What is deep learning?\", top_k_retriever=10, top_k_reader=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[   {   'answer': 'Keras',\n        'context': \"oss')\\n\"\n                   \"plt.xlabel('epoch')\\n\"\n                   \"plt.legend(['train loss', 'validation loss'], loc='upper \"\n                   \"right')\\n\"\n                   '\\x0c'\n                   'Chapter 3: Deep Learning with Keras | 337\\n'\n                   'Expected output:',\n        'score': 6.224386215209961},\n    {   'answer': 'cyclical: 284',\n        'context': ', 254, 261, 263,\\n'\n                   '266, 269, 271, 274,\\n'\n                   '276, 294, 298, 301,\\n'\n                   '305-306, 308-309\\n'\n                   'current: 95, 113, 283,\\n'\n                   '287, 294, 301\\n'\n                   'curves: 193-194, 204, 222\\n'\n                   'cyclical: 284',\n        'score': -4.464661598205566},\n    {   'answer': 'gradient',\n        'context': '237, 248, 250, 259,\\n'\n                   '268, 292, 298, 304\\n'\n                   'google: 258, 277,\\n'\n                   '282-283, 286, 308\\n'\n                   'gradient: 78, 84, 94-96,\\n'\n                   '114, 118, 277, 279-280,\\n'\n                   '287-289, 291-292, 308',\n        'score': -4.519716739654541}]\n"
     ]
    }
   ],
   "source": [
    "print_answers(prediction, details=\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.78 Batches/s]\n",
      "12/09/2020 11:09:30 - INFO - haystack.finder -   Got 10 candidates from retriever\n",
      "12/09/2020 11:09:30 - INFO - haystack.finder -   Reader is looking for detailed answer in 9211 chars ...\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.62s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.08 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.37s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.26 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.13 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.93 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.03 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.64 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.36 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.40 Batches/s]\n"
     ]
    }
   ],
   "source": [
    "prediction = finder.get_answers(question=\"What is logistic regression?\", top_k_retriever=10, top_k_reader=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[   {   'answer': 'feature\\n'\n                  'coefficients are learned and predictions are made by taking '\n                  'the sum of the product\\n'\n                  'of the feature coefficients and features',\n        'context': ' in which feature\\n'\n                   'coefficients are learned and predictions are made by '\n                   'taking the sum of the product\\n'\n                   'of the feature coefficients and features.\\n'\n                   '• Decis',\n        'score': 10.148681640625},\n    {   'answer': 'a very simple neural network with only one hidden layer and '\n                  'only\\n'\n                  'one node in its hidden layer',\n        'context': 'logistic\\n'\n                   'regression involves a very simple neural network with only '\n                   'one hidden layer and only\\n'\n                   'one node in its hidden layer.\\n'\n                   'An overview of the logisti',\n        'score': 10.00578498840332},\n    {   'answer': 'ridge and lasso regularization',\n        'context': 'techniques. For example, in\\n'\n                   'linear and logistic regression, ridge and lasso '\n                   'regularization are most common.\\n'\n                   'In tree-based models, limiting the maximum',\n        'score': 3.0434062480926514}]\n"
     ]
    }
   ],
   "source": [
    "print_answers(prediction, details=\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.10 Batches/s]\n",
      "12/09/2020 11:09:41 - INFO - haystack.finder -   Got 10 candidates from retriever\n",
      "12/09/2020 11:09:41 - INFO - haystack.finder -   Reader is looking for detailed answer in 6129 chars ...\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.26s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.02 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.12 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.27 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.19 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.92 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.52 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.03s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.17 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.24 Batches/s]\n"
     ]
    }
   ],
   "source": [
    "prediction = finder.get_answers(question=\"What is early stopping?\", top_k_retriever=10, top_k_reader=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[   {   'answer': 'forcing the Keras model to stop the training when a desired '\n                  'metric—for example,\\n'\n                  'the test error rate—is not improving anymore',\n        'context': '. This\\n'\n                   'means forcing the Keras model to stop the training when a '\n                   'desired metric—for example,\\n'\n                   'the test error rate—is not improving anymore. In order to',\n        'score': 10.376293182373047},\n    {   'answer': 'The training',\n        'context': 'ovide it as a callbacks\\n'\n                   'argument to model.fit() and train the model. The training '\n                   'will automatically stop\\n'\n                   'according to the EarlyStopping() callback:\\n'\n                   'h',\n        'score': 6.640634536743164},\n    {   'answer': 'the training\\n'\n                  'process automatically stops after about 150 epochs',\n        'context': 'ing callback with patience=10 to the model, the training\\n'\n                   'process automatically stops after about 150 epochs.\\n'\n                   '\\x0c'\n                   'Other Regularization Methods | 183\\n'\n                   'In th',\n        'score': 6.563176155090332}]\n"
     ]
    }
   ],
   "source": [
    "print_answers(prediction, details=\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.10 Batches/s]\n",
      "12/09/2020 11:09:52 - INFO - haystack.finder -   Got 10 candidates from retriever\n",
      "12/09/2020 11:09:52 - INFO - haystack.finder -   Reader is looking for detailed answer in 6771 chars ...\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.13s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.10 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.14 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.88 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.90 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.11 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.14 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.28 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.47 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.07 Batches/s]\n"
     ]
    }
   ],
   "source": [
    "prediction = finder.get_answers(question=\"What is early stopping used for?\", top_k_retriever=10, top_k_reader=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[   {   'answer': 'forcing the Keras model to stop the training when a desired '\n                  'metric—for example,\\n'\n                  'the test error rate—is not improving anymore',\n        'context': '. This\\n'\n                   'means forcing the Keras model to stop the training when a '\n                   'desired metric—for example,\\n'\n                   'the test error rate—is not improving anymore. In order to',\n        'score': 8.451149940490723},\n    {   'answer': 'to prevent your Keras model from\\n'\n                  'overfitting the training data. To do this, you utilized the '\n                  'EarlyStopping callback\\n'\n                  'and trained the model with it. We used this callback to '\n                  'stop the model any time the\\n'\n                  'validation loss increased',\n        'context': ' to prevent your Keras model from\\n'\n                   'overfitting the training data. To do this, you utilized '\n                   'the EarlyStopping callback\\n'\n                   'and trained the model with it. We used this callback to '\n                   'stop the model any time the\\n'\n                   'validation loss increased',\n        'score': 8.032074928283691},\n    {   'answer': 'training',\n        'context': 'ide it as a callbacks\\n'\n                   'argument to model.fit() and train the model. The training '\n                   'will automatically stop\\n'\n                   'according to the EarlyStopping() callback:\\n'\n                   'his',\n        'score': 7.87277889251709}]\n"
     ]
    }
   ],
   "source": [
    "print_answers(prediction, details=\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_index = \"evaluation_docs\"\n",
    "label_index = \"evaluation_labels\"\n",
    "\n",
    "filename=\"./data/nq/nq_dev_subset_v2.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.preprocessor.utils import eval_data_from_file\n",
    "docs, labels = eval_data_from_file(filename=filename) # return: (List of Documents, List of Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.document_store.faiss import FAISSDocumentStore\n",
    "document_store = FAISSDocumentStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store.write_documents(docs, index=doc_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "12/09/2020 11:11:07 - WARNING - farm.modeling.language_model -   Could not automatically detect from language model name what language it is. \n",
      "\t We guess it's an *ENGLISH* model ... \n",
      "\t If not: Init the language model by supplying the 'language' param.\n",
      "12/09/2020 11:11:14 - WARNING - farm.modeling.language_model -   Could not automatically detect from language model name what language it is. \n",
      "\t We guess it's an *ENGLISH* model ... \n",
      "\t If not: Init the language model by supplying the 'language' param.\n"
     ]
    }
   ],
   "source": [
    "from haystack.retriever.dense import DensePassageRetriever\n",
    "dpr = DensePassageRetriever(document_store=document_store,\n",
    "                                  query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n",
    "                                  passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\n",
    "                                  max_seq_len_query=64,\n",
    "                                  max_seq_len_passage=256,\n",
    "                                  batch_size=16,\n",
    "                                  use_gpu=False,\n",
    "                                  embed_title=True,\n",
    "                                  use_fast_tokenizers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "12/09/2020 11:11:17 - INFO - haystack.document_store.faiss -   Updating embeddings for 50 docs...\n",
      "Inferencing Samples: 100%|██████████| 4/4 [00:21<00:00,  5.42s/ Batches]\n",
      "12/09/2020 11:11:40 - INFO - haystack.document_store.faiss -   Indexing embeddings and updating vectors_ids...\n",
      "100%|██████████| 1/1 [00:00<00:00, 93.36it/s]\n"
     ]
    }
   ],
   "source": [
    "document_store.update_embeddings(dpr, index=doc_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store.write_labels(labels, index=label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "12/09/2020 11:12:29 - INFO - haystack.retriever.base -   Performing eval queries...\n",
      "  0%|          | 0/54 [00:00<?, ?it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.94 Batches/s]\n",
      "  2%|▏         | 1/54 [00:00<00:12,  4.40it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.00 Batches/s]\n",
      "  4%|▎         | 2/54 [00:00<00:11,  4.64it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.05 Batches/s]\n",
      "  6%|▌         | 3/54 [00:00<00:10,  4.90it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.01 Batches/s]\n",
      "  7%|▋         | 4/54 [00:00<00:09,  5.03it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.96 Batches/s]\n",
      "  9%|▉         | 5/54 [00:00<00:09,  5.17it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.60 Batches/s]\n",
      " 11%|█         | 6/54 [00:01<00:09,  5.17it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.36 Batches/s]\n",
      " 13%|█▎        | 7/54 [00:01<00:09,  5.09it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.57 Batches/s]\n",
      " 15%|█▍        | 8/54 [00:01<00:09,  4.79it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.18 Batches/s]\n",
      " 17%|█▋        | 9/54 [00:01<00:09,  4.76it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.96 Batches/s]\n",
      " 19%|█▊        | 10/54 [00:01<00:08,  4.97it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.61 Batches/s]\n",
      " 20%|██        | 11/54 [00:02<00:09,  4.76it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.78 Batches/s]\n",
      " 22%|██▏       | 12/54 [00:02<00:09,  4.64it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.59 Batches/s]\n",
      " 24%|██▍       | 13/54 [00:02<00:09,  4.54it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.57 Batches/s]\n",
      " 26%|██▌       | 14/54 [00:02<00:08,  4.72it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.61 Batches/s]\n",
      " 28%|██▊       | 15/54 [00:03<00:08,  4.56it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.09 Batches/s]\n",
      " 30%|██▉       | 16/54 [00:03<00:08,  4.26it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.06 Batches/s]\n",
      " 31%|███▏      | 17/54 [00:03<00:08,  4.13it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.17 Batches/s]\n",
      " 33%|███▎      | 18/54 [00:03<00:08,  4.08it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.48 Batches/s]\n",
      " 35%|███▌      | 19/54 [00:04<00:08,  4.12it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.51 Batches/s]\n",
      " 37%|███▋      | 20/54 [00:04<00:08,  4.08it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.54 Batches/s]\n",
      " 39%|███▉      | 21/54 [00:04<00:07,  4.13it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.41 Batches/s]\n",
      " 41%|████      | 22/54 [00:04<00:07,  4.31it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.57 Batches/s]\n",
      " 43%|████▎     | 23/54 [00:05<00:07,  4.29it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.88 Batches/s]\n",
      " 44%|████▍     | 24/54 [00:05<00:06,  4.34it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.68 Batches/s]\n",
      " 46%|████▋     | 25/54 [00:05<00:06,  4.57it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.01 Batches/s]\n",
      " 48%|████▊     | 26/54 [00:05<00:06,  4.58it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.72 Batches/s]\n",
      " 50%|█████     | 27/54 [00:05<00:06,  4.50it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.25 Batches/s]\n",
      " 52%|█████▏    | 28/54 [00:06<00:05,  4.57it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.39 Batches/s]\n",
      " 54%|█████▎    | 29/54 [00:06<00:05,  4.40it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.75 Batches/s]\n",
      " 56%|█████▌    | 30/54 [00:06<00:05,  4.39it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.88 Batches/s]\n",
      " 57%|█████▋    | 31/54 [00:06<00:04,  4.68it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.30 Batches/s]\n",
      " 59%|█████▉    | 32/54 [00:06<00:04,  4.76it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.75 Batches/s]\n",
      " 61%|██████    | 33/54 [00:07<00:04,  4.91it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.71 Batches/s]\n",
      " 63%|██████▎   | 34/54 [00:07<00:03,  5.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.23 Batches/s]\n",
      " 65%|██████▍   | 35/54 [00:07<00:03,  4.97it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.34 Batches/s]\n",
      " 67%|██████▋   | 36/54 [00:07<00:03,  4.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.16 Batches/s]\n",
      " 69%|██████▊   | 37/54 [00:07<00:03,  4.87it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.72 Batches/s]\n",
      " 70%|███████   | 38/54 [00:08<00:03,  4.95it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.15 Batches/s]\n",
      " 72%|███████▏  | 39/54 [00:08<00:03,  4.90it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.61 Batches/s]\n",
      " 74%|███████▍  | 40/54 [00:08<00:02,  4.89it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.79 Batches/s]\n",
      " 76%|███████▌  | 41/54 [00:08<00:02,  5.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.53 Batches/s]\n",
      " 78%|███████▊  | 42/54 [00:08<00:02,  5.07it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.57 Batches/s]\n",
      " 80%|███████▉  | 43/54 [00:09<00:02,  4.82it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.40 Batches/s]\n",
      " 81%|████████▏ | 44/54 [00:09<00:02,  4.56it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.58 Batches/s]\n",
      " 83%|████████▎ | 45/54 [00:09<00:01,  4.73it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.50 Batches/s]\n",
      " 85%|████████▌ | 46/54 [00:09<00:01,  4.74it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.58 Batches/s]\n",
      " 87%|████████▋ | 47/54 [00:10<00:01,  4.23it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.99 Batches/s]\n",
      " 89%|████████▉ | 48/54 [00:10<00:01,  4.06it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.99 Batches/s]\n",
      " 91%|█████████ | 49/54 [00:10<00:01,  4.18it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.39 Batches/s]\n",
      " 93%|█████████▎| 50/54 [00:10<00:00,  4.37it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.45 Batches/s]\n",
      " 94%|█████████▍| 51/54 [00:11<00:00,  4.50it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.32 Batches/s]\n",
      " 96%|█████████▋| 52/54 [00:11<00:00,  4.59it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.33 Batches/s]\n",
      " 98%|█████████▊| 53/54 [00:11<00:00,  4.67it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.75 Batches/s]\n",
      "100%|██████████| 54/54 [00:11<00:00,  4.59it/s]\n",
      "12/09/2020 11:12:41 - INFO - haystack.retriever.base -   For 52 out of 54 questions (96.30%), the answer was in the top-5 candidate passages selected by the retriever.\n"
     ]
    }
   ],
   "source": [
    "## Evaluate Retriever on its own\n",
    "dpr_eval_results = dpr.eval(top_k=5, label_index=label_index, doc_index=doc_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "12/09/2020 11:13:12 - INFO - haystack.retriever.base -   Performing eval queries...\n",
      "  0%|          | 0/54 [00:00<?, ?it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.72 Batches/s]\n",
      "  2%|▏         | 1/54 [00:00<00:10,  5.26it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.84 Batches/s]\n",
      "  4%|▎         | 2/54 [00:00<00:09,  5.27it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.78 Batches/s]\n",
      "  6%|▌         | 3/54 [00:00<00:09,  5.28it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.86 Batches/s]\n",
      "  7%|▋         | 4/54 [00:00<00:09,  5.34it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.54 Batches/s]\n",
      "  9%|▉         | 5/54 [00:00<00:09,  5.29it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.02 Batches/s]\n",
      " 11%|█         | 6/54 [00:01<00:09,  5.11it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.95 Batches/s]\n",
      " 13%|█▎        | 7/54 [00:01<00:09,  5.17it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.62 Batches/s]\n",
      " 15%|█▍        | 8/54 [00:01<00:10,  4.49it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.46 Batches/s]\n",
      " 17%|█▋        | 9/54 [00:01<00:10,  4.40it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.51 Batches/s]\n",
      " 19%|█▊        | 10/54 [00:02<00:10,  4.36it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.84 Batches/s]\n",
      " 20%|██        | 11/54 [00:02<00:09,  4.44it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.00 Batches/s]\n",
      " 22%|██▏       | 12/54 [00:02<00:08,  4.68it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.14 Batches/s]\n",
      " 24%|██▍       | 13/54 [00:02<00:08,  4.96it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.35 Batches/s]\n",
      " 26%|██▌       | 14/54 [00:02<00:07,  5.19it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.14 Batches/s]\n",
      " 28%|██▊       | 15/54 [00:03<00:07,  5.29it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.67 Batches/s]\n",
      " 30%|██▉       | 16/54 [00:03<00:07,  5.01it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.60 Batches/s]\n",
      " 31%|███▏      | 17/54 [00:03<00:07,  4.78it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.97 Batches/s]\n",
      " 33%|███▎      | 18/54 [00:03<00:07,  4.93it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.82 Batches/s]\n",
      " 35%|███▌      | 19/54 [00:03<00:06,  5.05it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.36 Batches/s]\n",
      " 37%|███▋      | 20/54 [00:04<00:06,  4.94it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.35 Batches/s]\n",
      " 39%|███▉      | 21/54 [00:04<00:06,  4.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.38 Batches/s]\n",
      " 41%|████      | 22/54 [00:04<00:06,  4.69it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.33 Batches/s]\n",
      " 43%|████▎     | 23/54 [00:04<00:06,  4.67it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.84 Batches/s]\n",
      " 44%|████▍     | 24/54 [00:04<00:06,  4.63it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.75 Batches/s]\n",
      " 46%|████▋     | 25/54 [00:05<00:06,  4.79it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.05 Batches/s]\n",
      " 48%|████▊     | 26/54 [00:05<00:05,  5.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.85 Batches/s]\n",
      " 50%|█████     | 27/54 [00:05<00:05,  5.07it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.38 Batches/s]\n",
      " 52%|█████▏    | 28/54 [00:05<00:05,  5.03it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.78 Batches/s]\n",
      " 54%|█████▎    | 29/54 [00:05<00:05,  4.83it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.62 Batches/s]\n",
      " 56%|█████▌    | 30/54 [00:06<00:04,  4.95it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.73 Batches/s]\n",
      " 57%|█████▋    | 31/54 [00:06<00:04,  5.08it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.48 Batches/s]\n",
      " 59%|█████▉    | 32/54 [00:06<00:04,  5.09it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.99 Batches/s]\n",
      " 61%|██████    | 33/54 [00:06<00:04,  5.20it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.90 Batches/s]\n",
      " 63%|██████▎   | 34/54 [00:06<00:03,  5.30it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.96 Batches/s]\n",
      " 65%|██████▍   | 35/54 [00:07<00:03,  5.37it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.74 Batches/s]\n",
      " 67%|██████▋   | 36/54 [00:07<00:03,  5.21it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.05 Batches/s]\n",
      " 69%|██████▊   | 37/54 [00:07<00:03,  5.04it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.63 Batches/s]\n",
      " 70%|███████   | 38/54 [00:07<00:03,  4.81it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.07 Batches/s]\n",
      " 72%|███████▏  | 39/54 [00:07<00:03,  4.43it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.76 Batches/s]\n",
      " 74%|███████▍  | 40/54 [00:08<00:02,  4.68it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.62 Batches/s]\n",
      " 76%|███████▌  | 41/54 [00:08<00:02,  4.82it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.14 Batches/s]\n",
      " 78%|███████▊  | 42/54 [00:08<00:02,  4.75it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.88 Batches/s]\n",
      " 80%|███████▉  | 43/54 [00:08<00:02,  4.96it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.76 Batches/s]\n",
      " 81%|████████▏ | 44/54 [00:08<00:01,  5.06it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.17 Batches/s]\n",
      " 83%|████████▎ | 45/54 [00:09<00:01,  4.99it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.53 Batches/s]\n",
      " 85%|████████▌ | 46/54 [00:09<00:01,  4.37it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.53 Batches/s]\n",
      " 87%|████████▋ | 47/54 [00:09<00:01,  4.30it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.52 Batches/s]\n",
      " 89%|████████▉ | 48/54 [00:09<00:01,  4.48it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.91 Batches/s]\n",
      " 91%|█████████ | 49/54 [00:10<00:01,  4.45it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.47 Batches/s]\n",
      " 93%|█████████▎| 50/54 [00:10<00:00,  4.41it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.27 Batches/s]\n",
      " 94%|█████████▍| 51/54 [00:10<00:00,  4.29it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.68 Batches/s]\n",
      " 96%|█████████▋| 52/54 [00:10<00:00,  4.31it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.50 Batches/s]\n",
      " 98%|█████████▊| 53/54 [00:11<00:00,  4.27it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.56 Batches/s]\n",
      "100%|██████████| 54/54 [00:11<00:00,  4.78it/s]\n",
      "12/09/2020 11:13:23 - INFO - haystack.retriever.base -   For 52 out of 54 questions (96.30%), the answer was in the top-3 candidate passages selected by the retriever.\n"
     ]
    }
   ],
   "source": [
    "## Evaluate Retriever on its own\n",
    "dpr_eval_results = dpr.eval(top_k=3, label_index=label_index, doc_index=doc_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Retriever Recall: 0.9629629629629629\nRetriever Mean Avg Precision: 0.9537037037037037\n"
     ]
    }
   ],
   "source": [
    "## Retriever Recall is the proportion of questions for which the correct document containing the answer is\n",
    "## among the correct documents\n",
    "print(\"Retriever Recall:\", dpr_eval_results[\"recall\"])\n",
    "## Retriever Mean Avg Precision rewards retrievers that give relevant documents a higher rank\n",
    "print(\"Retriever Mean Avg Precision:\", dpr_eval_results[\"map\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "12/09/2020 11:13:37 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n",
      "12/09/2020 11:13:37 - INFO - farm.infer -   Could not find `deepset/roberta-base-squad2` locally. Try to download from model hub ...\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at deepset/roberta-base-squad2 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "12/09/2020 11:13:42 - WARNING - farm.modeling.language_model -   Could not automatically detect from language model name what language it is. \n",
      "\t We guess it's an *ENGLISH* model ... \n",
      "\t If not: Init the language model by supplying the 'language' param.\n",
      "12/09/2020 11:13:51 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n",
      "12/09/2020 11:13:51 - INFO - farm.infer -   Got ya 3 parallel workers to do inference ...\n",
      "12/09/2020 11:13:51 - INFO - farm.infer -    0    0    0 \n",
      "12/09/2020 11:13:51 - INFO - farm.infer -   /w\\  /w\\  /w\\\n",
      "12/09/2020 11:13:51 - INFO - farm.infer -   /'\\  / \\  /'\\\n",
      "12/09/2020 11:13:52 - INFO - farm.infer -       \n"
     ]
    }
   ],
   "source": [
    "from haystack.reader.farm import FARMReader\n",
    "farm_reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "12/09/2020 11:17:26 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n"
     ]
    }
   ],
   "source": [
    "from farm.utils import initialize_device_settings\n",
    "\n",
    "device, n_gpu = initialize_device_settings(use_cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "12/09/2020 11:17:35 - INFO - haystack.reader.farm -   Performing Evaluation using top_k_per_candidate = 3 \n",
      "and consequently, QuestionAnsweringPredictionHead.n_best = 4. \n",
      "This deviates from FARM's default where QuestionAnsweringPredictionHead.n_best = 5\n",
      "Evaluating: 100%|██████████| 73/73 [27:45<00:00, 22.82s/it]Reader Top-N-Accuracy: 0.6111111111111112\n",
      "Reader Exact Match: 0.2777777777777778\n",
      "Reader F1-Score: 0.30750487329434695\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Reader on its own\n",
    "reader_eval_results = farm_reader.eval(document_store=document_store, device=device, label_index=label_index, doc_index=doc_index)\n",
    "# Evaluation of Reader can also be done directly on a SQuAD-formatted file without passing the data to Elasticsearch\n",
    "#reader_eval_results = reader.eval_on_file(\"../data/nq\", \"nq_dev_subset_v2.json\", device=device)\n",
    "\n",
    "## Reader Top-N-Accuracy is the proportion of predicted answers that match with their corresponding correct answer\n",
    "print(\"Reader Top-N-Accuracy:\", reader_eval_results[\"top_n_accuracy\"])\n",
    "## Reader Exact Match is the proportion of questions where the predicted answer is exactly the same as the correct answer\n",
    "print(\"Reader Exact Match:\", reader_eval_results[\"EM\"])\n",
    "## Reader F1-Score is the average overlap between the predicted answers and the correct answers\n",
    "print(\"Reader F1-Score:\", reader_eval_results[\"f1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}