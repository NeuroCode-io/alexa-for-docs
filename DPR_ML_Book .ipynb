{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.1 64-bit ('.venv')",
   "metadata": {
    "interpreter": {
     "hash": "5bdca235408bc63495fd8719d073449338626e85bb6ce08bacdb45b63101d775"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "12/02/2020 13:19:53 - INFO - faiss -   Loading faiss with AVX2 support.\n",
      "12/02/2020 13:19:53 - INFO - faiss -   Loading faiss.\n"
     ]
    }
   ],
   "source": [
    "from haystack.preprocessor.utils import convert_files_to_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "12/02/2020 13:19:55 - INFO - haystack.preprocessor.utils -   Converting /home/elena/Downloads/data/9781839217579-THE_DEEP_LEARNING_WITH_KERAS_WORKSHOP_SECOND_EDITION.pdf\n"
     ]
    }
   ],
   "source": [
    "dicts=convert_files_to_dicts(\"/home/elena/Downloads/data/\", split_paragraphs=True) # no cleaning function applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(list, 1418, dict, dict_keys(['text', 'meta']))"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "type(dicts), len(dicts), type(dicts[0]), dicts[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'text': '\\x0cThe Deep Learning\\nwith Keras Workshop\\nSecond Edition', 'meta': {'name': '9781839217579-THE_DEEP_LEARNING_WITH_KERAS_WORKSHOP_SECOND_EDITION.pdf'}}\n"
     ]
    }
   ],
   "source": [
    "print(dicts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.document_store.faiss import FAISSDocumentStore\n",
    "\n",
    "document_store = FAISSDocumentStore()\n",
    "document_store.delete_all_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store.write_documents(dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "12/02/2020 13:21:26 - WARNING - farm.modeling.language_model -   Could not automatically detect from language model name what language it is. \n",
      "\t We guess it's an *ENGLISH* model ... \n",
      "\t If not: Init the language model by supplying the 'language' param.\n",
      "12/02/2020 13:21:33 - WARNING - farm.modeling.language_model -   Could not automatically detect from language model name what language it is. \n",
      "\t We guess it's an *ENGLISH* model ... \n",
      "\t If not: Init the language model by supplying the 'language' param.\n"
     ]
    }
   ],
   "source": [
    "from haystack.retriever.dense import DensePassageRetriever\n",
    "retriever = DensePassageRetriever(document_store=document_store,\n",
    "                                  query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n",
    "                                  passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\n",
    "                                  max_seq_len_query=64,\n",
    "                                  max_seq_len_passage=256,\n",
    "                                  batch_size=16,\n",
    "                                  use_gpu=False,\n",
    "                                  embed_title=True,\n",
    "                                  use_fast_tokenizers=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "12/02/2020 13:21:38 - INFO - haystack.document_store.faiss -   Updating embeddings for 1418 docs...\n",
      "Inferencing Samples: 100%|██████████| 89/89 [12:37<00:00,  8.51s/ Batches]\n",
      "12/02/2020 13:34:18 - INFO - haystack.document_store.faiss -   Indexing embeddings and updating vectors_ids...\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.84it/s]\n"
     ]
    }
   ],
   "source": [
    "document_store.update_embeddings(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "12/02/2020 13:34:30 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n",
      "12/02/2020 13:34:30 - INFO - farm.infer -   Could not find `deepset/roberta-base-squad2` locally. Try to download from model hub ...\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at deepset/roberta-base-squad2 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "12/02/2020 13:34:34 - WARNING - farm.modeling.language_model -   Could not automatically detect from language model name what language it is. \n",
      "\t We guess it's an *ENGLISH* model ... \n",
      "\t If not: Init the language model by supplying the 'language' param.\n",
      "12/02/2020 13:34:43 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n",
      "12/02/2020 13:34:43 - INFO - farm.infer -   Got ya 3 parallel workers to do inference ...\n",
      "12/02/2020 13:34:43 - INFO - farm.infer -    0    0    0 \n",
      "12/02/2020 13:34:43 - INFO - farm.infer -   /w\\  /w\\  /w\\\n",
      "12/02/2020 13:34:43 - INFO - farm.infer -   /'\\  / \\  /'\\\n",
      "12/02/2020 13:34:43 - INFO - farm.infer -       \n"
     ]
    }
   ],
   "source": [
    "from haystack.reader.farm import FARMReader\n",
    "farm_reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder = Finder(farm_reader, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.12 Batches/s]\n",
      "12/02/2020 13:43:43 - INFO - haystack.finder -   Got 10 candidates from retriever\n",
      "12/02/2020 13:43:43 - INFO - haystack.finder -   Reader is looking for detailed answer in 3458 chars ...\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.12 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.11 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.19 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.56 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.85 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.03 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.19 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.13 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.65 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.52 Batches/s]\n"
     ]
    }
   ],
   "source": [
    "prediction = finder.get_answers(question=\"What is RNN?\", top_k_retriever=10, top_k_reader=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.utils import print_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[   {   'answer': 'the hidden layer not only gives the\\n'\n                  'output, but it also feeds back the information of the '\n                  'output into itself',\n        'context': 'y of the RNN is that the hidden layer not only gives the\\n'\n                   'output, but it also feeds back the information of the '\n                   'output into itself. Before taking a\\n'\n                   'dee',\n        'score': 12.397671699523926},\n    {   'answer': 'Recurrent Neural Networks',\n        'context': 'Recurrent Neural Networks (RNNs)\\n'\n                   'RNNs are a class of neural networks that are built on the '\n                   'concept of sequential\\n'\n                   'memory. Unlike traditional neural net',\n        'score': 11.025992393493652},\n    {   'answer': 'Long Short-Term Memory',\n        'context': 'Long Short-Term Memory (LSTM)\\n'\n                   'LSTMs are RNNs whose main objective is to overcome the '\n                   'shortcomings of the vanishing\\n'\n                   'gradient and exploding gradient pro',\n        'score': 4.55922269821167}]\n"
     ]
    }
   ],
   "source": [
    "print_answers(prediction, details=\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.93 Batches/s]\n",
      "12/02/2020 13:43:58 - INFO - haystack.finder -   Got 10 candidates from retriever\n",
      "12/02/2020 13:43:58 - INFO - haystack.finder -   Reader is looking for detailed answer in 5180 chars ...\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.07s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.24s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.12 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.78s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.43 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.36 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.29 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.08 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.02 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.17 Batches/s]\n"
     ]
    }
   ],
   "source": [
    "prediction = finder.get_answers(question=\"What is a layer?\", top_k_retriever=10, top_k_reader=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[   {   'answer': 'Dense layer',\n        'context': 'ras. For now, we will use only the simplest type of\\n'\n                   'layer, called the Dense layer. A Dense layer is equivalent '\n                   'to the fully connected\\n'\n                   'layers that we h',\n        'score': 10.62425708770752},\n    {   'answer': 'a\\ncomposition of nodes',\n        'context': 'ers is part of the Keras core API. A layer can be thought '\n                   'of as a\\n'\n                   'composition of nodes, and at each node, a set of '\n                   'computations happen. In Keras, all ',\n        'score': 10.560615539550781},\n    {   'answer': 'convolutional',\n        'context': 'd: it has a four-dimensional input shape (None, 224, 224,\\n'\n                   '3) and it has three convolutional layers.\\n'\n                   'The last four layers of the output are as follows:',\n        'score': 9.095014572143555}]\n"
     ]
    }
   ],
   "source": [
    "print_answers(prediction, details=\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.69 Batches/s]\n",
      "12/02/2020 13:44:20 - INFO - haystack.finder -   Got 10 candidates from retriever\n",
      "12/02/2020 13:44:20 - INFO - haystack.finder -   Reader is looking for detailed answer in 3292 chars ...\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.17s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.17s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.23 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.27 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.12 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.13 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.22 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.13 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.55s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.28 Batches/s]\n"
     ]
    }
   ],
   "source": [
    "prediction = finder.get_answers(question=\"What is deep learning?\", top_k_retriever=10, top_k_reader=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[   {   'answer': 'Keras',\n        'context': \"oss')\\n\"\n                   \"plt.xlabel('epoch')\\n\"\n                   \"plt.legend(['train loss', 'validation loss'], loc='upper \"\n                   \"right')\\n\"\n                   '\\x0c'\n                   'Chapter 3: Deep Learning with Keras | 337\\n'\n                   'Expected output:',\n        'score': 6.224386215209961},\n    {   'answer': 'cyclical: 284',\n        'context': ', 254, 261, 263,\\n'\n                   '266, 269, 271, 274,\\n'\n                   '276, 294, 298, 301,\\n'\n                   '305-306, 308-309\\n'\n                   'current: 95, 113, 283,\\n'\n                   '287, 294, 301\\n'\n                   'curves: 193-194, 204, 222\\n'\n                   'cyclical: 284',\n        'score': -4.464661598205566},\n    {   'answer': 'gradient',\n        'context': '237, 248, 250, 259,\\n'\n                   '268, 292, 298, 304\\n'\n                   'google: 258, 277,\\n'\n                   '282-283, 286, 308\\n'\n                   'gradient: 78, 84, 94-96,\\n'\n                   '114, 118, 277, 279-280,\\n'\n                   '287-289, 291-292, 308',\n        'score': -4.519716739654541}]\n"
     ]
    }
   ],
   "source": [
    "print_answers(prediction, details=\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.47 Batches/s]\n",
      "12/02/2020 13:44:39 - INFO - haystack.finder -   Got 10 candidates from retriever\n",
      "12/02/2020 13:44:39 - INFO - haystack.finder -   Reader is looking for detailed answer in 9211 chars ...\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.48s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.24 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.23s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.24 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.05s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.97 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.97 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.20 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.04 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.85 Batches/s]\n"
     ]
    }
   ],
   "source": [
    "prediction = finder.get_answers(question=\"What is logistic regression?\", top_k_retriever=10, top_k_reader=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[   {   'answer': 'feature\\n'\n                  'coefficients are learned and predictions are made by taking '\n                  'the sum of the product\\n'\n                  'of the feature coefficients and features',\n        'context': ' in which feature\\n'\n                   'coefficients are learned and predictions are made by '\n                   'taking the sum of the product\\n'\n                   'of the feature coefficients and features.\\n'\n                   '• Decis',\n        'score': 10.148681640625},\n    {   'answer': 'a very simple neural network with only one hidden layer and '\n                  'only\\n'\n                  'one node in its hidden layer',\n        'context': 'logistic\\n'\n                   'regression involves a very simple neural network with only '\n                   'one hidden layer and only\\n'\n                   'one node in its hidden layer.\\n'\n                   'An overview of the logisti',\n        'score': 10.00578498840332},\n    {   'answer': 'ridge and lasso regularization',\n        'context': 'techniques. For example, in\\n'\n                   'linear and logistic regression, ridge and lasso '\n                   'regularization are most common.\\n'\n                   'In tree-based models, limiting the maximum',\n        'score': 3.0434062480926514}]\n"
     ]
    }
   ],
   "source": [
    "print_answers(prediction, details=\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.62 Batches/s]\n",
      "12/02/2020 13:54:21 - INFO - haystack.finder -   Got 10 candidates from retriever\n",
      "12/02/2020 13:54:21 - INFO - haystack.finder -   Reader is looking for detailed answer in 6129 chars ...\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.00s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.48 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.35 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.94 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.03 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.08 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.08 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.07 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.24 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.17 Batches/s]\n"
     ]
    }
   ],
   "source": [
    "prediction = finder.get_answers(question=\"What is early stopping?\", top_k_retriever=10, top_k_reader=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[   {   'answer': 'forcing the Keras model to stop the training when a desired '\n                  'metric—for example,\\n'\n                  'the test error rate—is not improving anymore',\n        'context': '. This\\n'\n                   'means forcing the Keras model to stop the training when a '\n                   'desired metric—for example,\\n'\n                   'the test error rate—is not improving anymore. In order to',\n        'score': 10.376293182373047},\n    {   'answer': 'The training',\n        'context': 'ovide it as a callbacks\\n'\n                   'argument to model.fit() and train the model. The training '\n                   'will automatically stop\\n'\n                   'according to the EarlyStopping() callback:\\n'\n                   'h',\n        'score': 6.640634536743164},\n    {   'answer': 'the training\\n'\n                  'process automatically stops after about 150 epochs',\n        'context': 'ing callback with patience=10 to the model, the training\\n'\n                   'process automatically stops after about 150 epochs.\\n'\n                   '\\x0c'\n                   'Other Regularization Methods | 183\\n'\n                   'In th',\n        'score': 6.563176155090332}]\n"
     ]
    }
   ],
   "source": [
    "print_answers(prediction, details=\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.13 Batches/s]\n",
      "12/02/2020 13:54:52 - INFO - haystack.finder -   Got 10 candidates from retriever\n",
      "12/02/2020 13:54:52 - INFO - haystack.finder -   Reader is looking for detailed answer in 6771 chars ...\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.33s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.24 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.37 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.80 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.20 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.87 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.12 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.15 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.25 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.14 Batches/s]\n"
     ]
    }
   ],
   "source": [
    "prediction = finder.get_answers(question=\"What is early stopping used for?\", top_k_retriever=10, top_k_reader=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[   {   'answer': 'forcing the Keras model to stop the training when a desired '\n                  'metric—for example,\\n'\n                  'the test error rate—is not improving anymore',\n        'context': '. This\\n'\n                   'means forcing the Keras model to stop the training when a '\n                   'desired metric—for example,\\n'\n                   'the test error rate—is not improving anymore. In order to',\n        'score': 8.451149940490723},\n    {   'answer': 'to prevent your Keras model from\\n'\n                  'overfitting the training data. To do this, you utilized the '\n                  'EarlyStopping callback\\n'\n                  'and trained the model with it. We used this callback to '\n                  'stop the model any time the\\n'\n                  'validation loss increased',\n        'context': ' to prevent your Keras model from\\n'\n                   'overfitting the training data. To do this, you utilized '\n                   'the EarlyStopping callback\\n'\n                   'and trained the model with it. We used this callback to '\n                   'stop the model any time the\\n'\n                   'validation loss increased',\n        'score': 8.032074928283691},\n    {   'answer': 'training',\n        'context': 'ide it as a callbacks\\n'\n                   'argument to model.fit() and train the model. The training '\n                   'will automatically stop\\n'\n                   'according to the EarlyStopping() callback:\\n'\n                   'his',\n        'score': 7.87277889251709}]\n"
     ]
    }
   ],
   "source": [
    "print_answers(prediction, details=\"medium\")"
   ]
  }
 ]
}