{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_index = \"evaluation_docs\"\n",
    "label_index = \"evaluation_labels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"../data/nq/nq_dev_subset_v2.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "12/03/2020 14:03:29 - INFO - faiss -   Loading faiss with AVX2 support.\n",
      "12/03/2020 14:03:29 - INFO - faiss -   Loading faiss.\n"
     ]
    }
   ],
   "source": [
    "from haystack.preprocessor.utils import eval_data_from_file\n",
    "docs, labels = eval_data_from_file(filename=filename) # return: (List of Documents, List of Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(list, 50, haystack.schema.Document)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "type(docs), len(docs), type(docs[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(list, 96, haystack.schema.Label)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "type(labels), len(labels), type(labels[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.document_store.faiss import FAISSDocumentStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store = FAISSDocumentStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store.write_documents(docs, index=doc_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "12/03/2020 14:03:46 - WARNING - farm.modeling.language_model -   Could not automatically detect from language model name what language it is. \n",
      "\t We guess it's an *ENGLISH* model ... \n",
      "\t If not: Init the language model by supplying the 'language' param.\n",
      "12/03/2020 14:03:52 - WARNING - farm.modeling.language_model -   Could not automatically detect from language model name what language it is. \n",
      "\t We guess it's an *ENGLISH* model ... \n",
      "\t If not: Init the language model by supplying the 'language' param.\n"
     ]
    }
   ],
   "source": [
    "from haystack.retriever.dense import DensePassageRetriever\n",
    "dpr = DensePassageRetriever(document_store=document_store,\n",
    "                                  query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n",
    "                                  passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\n",
    "                                  max_seq_len_query=64,\n",
    "                                  max_seq_len_passage=256,\n",
    "                                  batch_size=16,\n",
    "                                  use_gpu=False,\n",
    "                                  embed_title=True,\n",
    "                                  use_fast_tokenizers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "12/03/2020 14:04:02 - INFO - haystack.document_store.faiss -   Updating embeddings for 50 docs...\n",
      "Inferencing Samples: 100%|██████████| 4/4 [00:22<00:00,  5.62s/ Batches]\n",
      "12/03/2020 14:04:25 - INFO - haystack.document_store.faiss -   Indexing embeddings and updating vectors_ids...\n",
      "100%|██████████| 1/1 [00:00<00:00, 98.46it/s]\n"
     ]
    }
   ],
   "source": [
    "document_store.update_embeddings(dpr, index=doc_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store.write_labels(labels, index=label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "12/03/2020 14:04:41 - INFO - haystack.retriever.base -   Performing eval queries...\n",
      "  0%|          | 0/54 [00:00<?, ?it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.95 Batches/s]\n",
      "  2%|▏         | 1/54 [00:00<00:14,  3.64it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.06 Batches/s]\n",
      "  4%|▎         | 2/54 [00:00<00:14,  3.68it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.96 Batches/s]\n",
      "  6%|▌         | 3/54 [00:00<00:12,  4.04it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.86 Batches/s]\n",
      "  7%|▋         | 4/54 [00:00<00:12,  4.16it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  7.54 Batches/s]\n",
      "  9%|▉         | 5/54 [00:01<00:10,  4.66it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  7.17 Batches/s]\n",
      " 11%|█         | 6/54 [00:01<00:09,  5.07it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.71 Batches/s]\n",
      " 13%|█▎        | 7/54 [00:01<00:09,  4.84it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.02 Batches/s]\n",
      " 15%|█▍        | 8/54 [00:01<00:09,  4.77it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  7.15 Batches/s]\n",
      " 17%|█▋        | 9/54 [00:01<00:08,  5.14it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.96 Batches/s]\n",
      " 19%|█▊        | 10/54 [00:02<00:08,  4.95it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  7.18 Batches/s]\n",
      " 20%|██        | 11/54 [00:02<00:08,  5.28it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.63 Batches/s]\n",
      " 22%|██▏       | 12/54 [00:02<00:07,  5.49it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.81 Batches/s]\n",
      " 24%|██▍       | 13/54 [00:02<00:07,  5.14it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.72 Batches/s]\n",
      " 26%|██▌       | 14/54 [00:02<00:08,  4.90it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.60 Batches/s]\n",
      " 28%|██▊       | 15/54 [00:03<00:07,  4.95it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.13 Batches/s]\n",
      " 30%|██▉       | 16/54 [00:03<00:07,  5.09it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.69 Batches/s]\n",
      " 31%|███▏      | 17/54 [00:03<00:06,  5.33it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.73 Batches/s]\n",
      " 33%|███▎      | 18/54 [00:03<00:07,  5.03it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.82 Batches/s]\n",
      " 35%|███▌      | 19/54 [00:03<00:07,  4.87it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.69 Batches/s]\n",
      " 37%|███▋      | 20/54 [00:04<00:07,  4.71it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.60 Batches/s]\n",
      " 39%|███▉      | 21/54 [00:04<00:07,  4.53it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.28 Batches/s]\n",
      " 41%|████      | 22/54 [00:04<00:06,  4.59it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.38 Batches/s]\n",
      " 43%|████▎     | 23/54 [00:04<00:06,  4.43it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.13 Batches/s]\n",
      " 44%|████▍     | 24/54 [00:04<00:06,  4.75it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.93 Batches/s]\n",
      " 46%|████▋     | 25/54 [00:05<00:05,  5.09it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.47 Batches/s]\n",
      " 48%|████▊     | 26/54 [00:05<00:05,  5.28it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.54 Batches/s]\n",
      " 50%|█████     | 27/54 [00:05<00:04,  5.45it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.68 Batches/s]\n",
      " 52%|█████▏    | 28/54 [00:05<00:05,  5.03it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.89 Batches/s]\n",
      " 54%|█████▎    | 29/54 [00:05<00:04,  5.16it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.32 Batches/s]\n",
      " 56%|█████▌    | 30/54 [00:06<00:04,  5.32it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.71 Batches/s]\n",
      " 57%|█████▋    | 31/54 [00:06<00:04,  4.97it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.57 Batches/s]\n",
      " 59%|█████▉    | 32/54 [00:06<00:04,  4.74it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.26 Batches/s]\n",
      " 61%|██████    | 33/54 [00:06<00:04,  4.96it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.38 Batches/s]\n",
      " 63%|██████▎   | 34/54 [00:06<00:03,  5.15it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.97 Batches/s]\n",
      " 65%|██████▍   | 35/54 [00:07<00:03,  5.17it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.92 Batches/s]\n",
      " 67%|██████▋   | 36/54 [00:07<00:03,  5.25it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.27 Batches/s]\n",
      " 69%|██████▊   | 37/54 [00:07<00:03,  5.34it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.21 Batches/s]\n",
      " 70%|███████   | 38/54 [00:07<00:02,  5.42it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.76 Batches/s]\n",
      " 72%|███████▏  | 39/54 [00:07<00:02,  5.33it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.10 Batches/s]\n",
      " 74%|███████▍  | 40/54 [00:08<00:02,  4.75it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.21 Batches/s]\n",
      " 76%|███████▌  | 41/54 [00:08<00:02,  4.94it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.22 Batches/s]\n",
      " 78%|███████▊  | 42/54 [00:08<00:02,  4.85it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.82 Batches/s]\n",
      " 80%|███████▉  | 43/54 [00:08<00:02,  4.75it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.81 Batches/s]\n",
      " 81%|████████▏ | 44/54 [00:08<00:02,  4.92it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.90 Batches/s]\n",
      " 83%|████████▎ | 45/54 [00:09<00:01,  5.09it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.90 Batches/s]\n",
      " 85%|████████▌ | 46/54 [00:09<00:01,  4.50it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.87 Batches/s]\n",
      " 87%|████████▋ | 47/54 [00:09<00:01,  4.74it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.65 Batches/s]\n",
      " 89%|████████▉ | 48/54 [00:09<00:01,  4.86it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.75 Batches/s]\n",
      " 91%|█████████ | 49/54 [00:09<00:01,  4.92it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.28 Batches/s]\n",
      " 93%|█████████▎| 50/54 [00:10<00:00,  4.56it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.42 Batches/s]\n",
      " 94%|█████████▍| 51/54 [00:10<00:00,  4.06it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.49 Batches/s]\n",
      " 96%|█████████▋| 52/54 [00:10<00:00,  3.26it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\n",
      " 98%|█████████▊| 53/54 [00:11<00:00,  3.11it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.21 Batches/s]\n",
      "100%|██████████| 54/54 [00:11<00:00,  4.67it/s]\n",
      "12/03/2020 14:04:53 - INFO - haystack.retriever.base -   For 54 out of 54 questions (100.00%), the answer was in the top-20 candidate passages selected by the retriever.\n"
     ]
    }
   ],
   "source": [
    "## Evaluate Retriever on its own\n",
    "dpr_eval_results = dpr.eval(top_k=20, label_index=label_index, doc_index=doc_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Retriever Recall: 1.0\nRetriever Mean Avg Precision: 0.9573045267489712\n"
     ]
    }
   ],
   "source": [
    "## Retriever Recall is the proportion of questions for which the correct document containing the answer is\n",
    "## among the correct documents\n",
    "print(\"Retriever Recall:\", dpr_eval_results[\"recall\"])\n",
    "## Retriever Mean Avg Precision rewards retrievers that give relevant documents a higher rank\n",
    "print(\"Retriever Mean Avg Precision:\", dpr_eval_results[\"map\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "12/03/2020 14:06:27 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n",
      "12/03/2020 14:06:27 - INFO - farm.infer -   Could not find `deepset/roberta-base-squad2` locally. Try to download from model hub ...\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at deepset/roberta-base-squad2 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "12/03/2020 14:06:32 - WARNING - farm.modeling.language_model -   Could not automatically detect from language model name what language it is. \n",
      "\t We guess it's an *ENGLISH* model ... \n",
      "\t If not: Init the language model by supplying the 'language' param.\n",
      "12/03/2020 14:06:41 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n",
      "12/03/2020 14:06:41 - INFO - farm.infer -   Got ya 3 parallel workers to do inference ...\n",
      "12/03/2020 14:06:41 - INFO - farm.infer -    0    0    0 \n",
      "12/03/2020 14:06:41 - INFO - farm.infer -   /w\\  /w\\  /w\\\n",
      "12/03/2020 14:06:41 - INFO - farm.infer -   /'\\  / \\  /'\\\n",
      "12/03/2020 14:06:41 - INFO - farm.infer -       \n"
     ]
    }
   ],
   "source": [
    "from haystack.reader.farm import FARMReader\n",
    "farm_reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Finder\n",
    "from haystack.utils import print_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder = Finder(farm_reader, dpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "12/03/2020 14:07:32 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n"
     ]
    }
   ],
   "source": [
    "from farm.utils import initialize_device_settings\n",
    "\n",
    "device, n_gpu = initialize_device_settings(use_cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "12/03/2020 14:07:42 - INFO - haystack.reader.farm -   Performing Evaluation using top_k_per_candidate = 3 \n",
      "and consequently, QuestionAnsweringPredictionHead.n_best = 4. \n",
      "This deviates from FARM's default where QuestionAnsweringPredictionHead.n_best = 5\n",
      "Evaluating: 100%|██████████| 73/73 [29:52<00:00, 24.55s/it]Reader Top-N-Accuracy: 0.6111111111111112\n",
      "Reader Exact Match: 0.2777777777777778\n",
      "Reader F1-Score: 0.30750487329434695\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Reader on its own\n",
    "reader_eval_results = farm_reader.eval(document_store=document_store, device=device, label_index=label_index, doc_index=doc_index)\n",
    "# Evaluation of Reader can also be done directly on a SQuAD-formatted file without passing the data to Elasticsearch\n",
    "#reader_eval_results = reader.eval_on_file(\"../data/nq\", \"nq_dev_subset_v2.json\", device=device)\n",
    "\n",
    "## Reader Top-N-Accuracy is the proportion of predicted answers that match with their corresponding correct answer\n",
    "print(\"Reader Top-N-Accuracy:\", reader_eval_results[\"top_n_accuracy\"])\n",
    "## Reader Exact Match is the proportion of questions where the predicted answer is exactly the same as the correct answer\n",
    "print(\"Reader Exact Match:\", reader_eval_results[\"EM\"])\n",
    "## Reader F1-Score is the average overlap between the predicted answers and the correct answers\n",
    "print(\"Reader F1-Score:\", reader_eval_results[\"f1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.50 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  7.24 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.65 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.95 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  7.51 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  7.51 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  7.30 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.01 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.88 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.92 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.90 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.16 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  7.29 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  7.41 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  7.14 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.01 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.72 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  7.00 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.64 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.15 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.12 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.57 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.35 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.57 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.55 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.00 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  7.48 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  7.35 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.62 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.66 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.67 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  7.54 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.63 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  8.51 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  8.18 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.75 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  7.84 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  7.97 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  8.35 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.84 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.71 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  7.86 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  8.02 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  8.02 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  7.95 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  8.12 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  7.71 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  8.09 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  8.28 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  8.36 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  8.11 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.93 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.39 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  7.75 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.97s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:06<00:00,  6.24s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 2/2 [00:36<00:00, 18.42s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:04<00:00,  4.06s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:23<00:00, 23.25s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:20<00:00, 20.96s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 2/2 [00:24<00:00, 12.50s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:23<00:00, 23.18s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:15<00:00, 15.82s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.83s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 2/2 [00:37<00:00, 18.76s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.13s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 2/2 [00:32<00:00, 16.46s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 2/2 [00:27<00:00, 13.66s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:22<00:00, 22.33s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 2/2 [00:41<00:00, 20.53s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 2/2 [00:28<00:00, 14.24s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 2/2 [00:28<00:00, 14.44s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:17<00:00, 17.47s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 5/5 [01:51<00:00, 22.37s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.19s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:26<00:00, 26.33s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 2/2 [00:28<00:00, 14.23s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 4/4 [01:33<00:00, 23.44s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:16<00:00, 16.37s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:23<00:00, 23.54s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.97s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:14<00:00, 14.10s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.07s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.11s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:16<00:00, 16.99s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 9/9 [03:15<00:00, 21.77s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 9/9 [03:16<00:00, 21.89s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:04<00:00,  4.19s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 2/2 [00:42<00:00, 21.24s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.88s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:20<00:00, 20.59s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:03<00:00,  3.95s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.55s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:22<00:00, 22.78s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:06<00:00,  6.72s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 2/2 [00:34<00:00, 17.15s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:05<00:00,  5.47s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 3/3 [00:58<00:00, 19.62s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.76s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 2/2 [00:50<00:00, 25.37s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:03<00:00,  3.90s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.74s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:16<00:00, 16.04s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 2/2 [00:24<00:00, 12.12s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 2/2 [00:24<00:00, 12.39s/ Batches]\n",
      "12/03/2020 15:15:12 - INFO - haystack.finder -   36.0 out of 54 questions were correctly answered 66.67%).\n",
      "12/03/2020 15:15:12 - INFO - haystack.finder -   3.0 questions could not be answered due to the retriever.\n",
      "12/03/2020 15:15:12 - INFO - haystack.finder -   15.0 questions could not be answered due to the reader.\n",
      "\n",
      "___Retriever Metrics in Finder___\n",
      "Retriever Recall            : 0.944\n",
      "Retriever Mean Avg Precision: 0.944\n",
      "Retriever Mean Reciprocal Rank: 0.944\n",
      "\n",
      "___Reader Metrics in Finder___\n",
      "Top-k accuracy\n",
      "Reader Top-1 accuracy             : 0.333\n",
      "Reader Top-1 accuracy (has answer): 0.143\n",
      "Reader Top-k accuracy             : 0.706\n",
      "Reader Top-k accuracy (has answer): 0.464\n",
      "Exact Match\n",
      "Reader Top-1 EM                   : 0.275\n",
      "Reader Top-1 EM (has answer)      : 0.036\n",
      "Reader Top-k EM                   : 0.588\n",
      "Reader Top-k EM (has answer)      : 0.250\n",
      "F1 score\n",
      "Reader Top-1 F1                   : 0.303\n",
      "Reader Top-1 F1 (has answer)      : 0.088\n",
      "Reader Top-k F1                   : 0.660\n",
      "Reader Top-k F1 (has answer)      : 0.380\n",
      "No Answer\n",
      "Reader Top-1 no-answer accuracy   : 0.565\n",
      "Reader Top-k no-answer accuracy   : 1.000\n",
      "\n",
      "___Time Measurements___\n",
      "Total retrieve time           : 9.106\n",
      "Avg retrieve time per question: 0.169\n",
      "Total reader timer            : 1573.617\n",
      "Avg read time per question    : 30.855\n",
      "Total Finder time             : 1582.728\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE Finder\n",
    "\n",
    "finder_eval_results = finder.eval(top_k_retriever=1, top_k_reader=10, label_index=label_index, doc_index=doc_index)\n",
    "finder.print_eval_results(finder_eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}